{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "mount_file_id": "1yhfT8RWjSWo4ynA3RZm-va2omGH4u2Jd",
      "authorship_tag": "ABX9TyNBlMAn4+P+DTaBaD2UX5WE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Drive - Data Set**"
      ],
      "metadata": {
        "id": "Zp0G3vhz8_m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8zMdGFmNEmq",
        "outputId": "becefd19-5bd3-4b48-a700-8b0376f58e17"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Clone Paper Code**"
      ],
      "metadata": {
        "id": "yywjDIOO9Nn4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ot8kIH-cIbM8",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbc19041-a213-4567-9230-4281f23ecadb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'SepReformer'...\n",
            "remote: Enumerating objects: 673, done.\u001b[K\n",
            "remote: Counting objects: 100% (173/173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 673 (delta 143), reused 126 (delta 126), pack-reused 500 (from 1)\u001b[K\n",
            "Receiving objects: 100% (673/673), 8.42 MiB | 18.08 MiB/s, done.\n",
            "Resolving deltas: 100% (416/416), done.\n",
            "/content/SepReformer\n",
            "Collecting absl-py==2.1.0 (from -r requirements.txt (line 1))\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting audioread==3.0.1 (from -r requirements.txt (line 2))\n",
            "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting certifi==2024.8.30 (from -r requirements.txt (line 3))\n",
            "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting cffi==1.17.1 (from -r requirements.txt (line 4))\n",
            "  Downloading cffi-1.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting charset-normalizer==3.4.0 (from -r requirements.txt (line 5))\n",
            "  Downloading charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Collecting contourpy==1.3.0 (from -r requirements.txt (line 6))\n",
            "  Downloading contourpy-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (0.12.1)\n",
            "Collecting decorator==5.1.1 (from -r requirements.txt (line 8))\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting filelock==3.16.1 (from -r requirements.txt (line 9))\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fonttools==4.54.1 (from -r requirements.txt (line 10))\n",
            "  Downloading fonttools-4.54.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.7/163.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec==2024.9.0 (from -r requirements.txt (line 11))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: future==1.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (1.0.0)\n",
            "Collecting grpcio==1.67.0 (from -r requirements.txt (line 13))\n",
            "  Downloading grpcio-1.67.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting idna==3.10 (from -r requirements.txt (line 14))\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting Jinja2==3.1.4 (from -r requirements.txt (line 15))\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting joblib==1.4.2 (from -r requirements.txt (line 16))\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting kiwisolver==1.4.7 (from -r requirements.txt (line 17))\n",
            "  Downloading kiwisolver-1.4.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: lazy_loader==0.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (0.4)\n",
            "Collecting librosa==0.10.2.post1 (from -r requirements.txt (line 19))\n",
            "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: llvmlite==0.43.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (0.43.0)\n",
            "Collecting loguru==0.7.2 (from -r requirements.txt (line 21))\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting Markdown==3.7 (from -r requirements.txt (line 22))\n",
            "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting MarkupSafe==3.0.1 (from -r requirements.txt (line 23))\n",
            "  Downloading MarkupSafe-3.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting matplotlib==3.9.2 (from -r requirements.txt (line 24))\n",
            "  Downloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting mir-eval==0.7 (from -r requirements.txt (line 25))\n",
            "  Downloading mir_eval-0.7.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 26)) (1.3.0)\n",
            "Collecting msgpack==1.1.0 (from -r requirements.txt (line 27))\n",
            "  Downloading msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting networkx==3.4.1 (from -r requirements.txt (line 28))\n",
            "  Downloading networkx-3.4.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 29)) (0.60.0)\n",
            "Collecting numpy==1.26.4 (from -r requirements.txt (line 30))\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from -r requirements.txt (line 31))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from -r requirements.txt (line 32))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from -r requirements.txt (line 33))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from -r requirements.txt (line 34))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from -r requirements.txt (line 35))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from -r requirements.txt (line 36))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from -r requirements.txt (line 37))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from -r requirements.txt (line 38))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from -r requirements.txt (line 39))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from -r requirements.txt (line 40))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.77 (from -r requirements.txt (line 41))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from -r requirements.txt (line 42))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting packaging==24.1 (from -r requirements.txt (line 43))\n",
            "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pandas==2.2.3 (from -r requirements.txt (line 44))\n",
            "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow==11.0.0 (from -r requirements.txt (line 45))\n",
            "  Downloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting platformdirs==4.3.6 (from -r requirements.txt (line 46))\n",
            "  Downloading platformdirs-4.3.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pooch==1.8.2 (from -r requirements.txt (line 47))\n",
            "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting protobuf==5.28.2 (from -r requirements.txt (line 48))\n",
            "  Downloading protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting ptflops==0.7.4 (from -r requirements.txt (line 49))\n",
            "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting pycparser==2.22 (from -r requirements.txt (line 50))\n",
            "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
            "Collecting pyparsing==3.2.0 (from -r requirements.txt (line 51))\n",
            "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 52)) (2.9.0.post0)\n",
            "Collecting pytz==2024.2 (from -r requirements.txt (line 53))\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting PyYAML==6.0.1 (from -r requirements.txt (line 54))\n",
            "  Downloading PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting requests==2.32.3 (from -r requirements.txt (line 55))\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting scikit-learn==1.5.2 (from -r requirements.txt (line 56))\n",
            "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting scipy==1.14.1 (from -r requirements.txt (line 57))\n",
            "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 58)) (0.13.2)\n",
            "Collecting six==1.16.0 (from -r requirements.txt (line 59))\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting soundfile==0.12.1 (from -r requirements.txt (line 60))\n",
            "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\n",
            "Collecting soxr==0.5.0.post1 (from -r requirements.txt (line 61))\n",
            "  Downloading soxr-0.5.0.post1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting sympy==1.13.3 (from -r requirements.txt (line 62))\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tensorboard==2.18.0 (from -r requirements.txt (line 63))\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tensorboard-data-server==0.7.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 64)) (0.7.2)\n",
            "Collecting thop==0.1.1.post2209072238 (from -r requirements.txt (line 65))\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting threadpoolctl==3.5.0 (from -r requirements.txt (line 66))\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.10.0 Requires-Python <3.12,>=3.8; 1.10.0rc1 Requires-Python <3.12,>=3.8; 1.10.0rc2 Requires-Python <3.12,>=3.8; 1.10.1 Requires-Python <3.12,>=3.8; 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11; 1.9.0 Requires-Python >=3.8,<3.12; 1.9.0rc1 Requires-Python >=3.8,<3.12; 1.9.0rc2 Requires-Python >=3.8,<3.12; 1.9.0rc3 Requires-Python >=3.8,<3.12; 1.9.1 Requires-Python >=3.8,<3.12\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.1.2 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0, 2.9.1, 2.10.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.1.2\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!git lfs install\n",
        "!git clone https://github.com/dmlguq456/SepReformer.git\n",
        "%cd SepReformer\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -r requirements.txt\n",
        "# Note: The repo recommends Python 3.10, which is currently the Colab standard."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "# Configuration\n",
        "base_path = '/content/drive/MyDrive/×ª×•××¨/speech'\n",
        "subfolders = ['s1', 's2', 'mix_clean']\n",
        "scp_output_dir = '/content/SepReformer/data/custom'\n",
        "os.makedirs(scp_output_dir, exist_ok=True)\n",
        "\n",
        "# 1. Get list of file names (assuming they are identical across s1, s2, and mix)\n",
        "# We sort them first to ensure indices match across all subfolders\n",
        "files = sorted([f for f in os.listdir(os.path.join(base_path, 's1')) if f.endswith('.wav')])\n",
        "\n",
        "# 2. Shuffle and Split (80/20)\n",
        "random.seed(42) # For reproducibility\n",
        "random.shuffle(files)\n",
        "split_idx = int(len(files) * 0.8)\n",
        "\n",
        "train_files = files[:split_idx]\n",
        "val_files = files[split_idx:]\n",
        "\n",
        "# 3. Generate the SCP files\n",
        "def create_scp(file_list, set_name):\n",
        "    for folder in subfolders:\n",
        "        output_path = os.path.join(scp_output_dir, f\"{set_name}_{folder}.scp\")\n",
        "        with open(output_path, 'w') as f:\n",
        "            for filename in file_list:\n",
        "                full_path = os.path.join(base_path, folder, filename)\n",
        "                # ID format: filename (must be unique)\n",
        "                f.write(f\"{filename} {full_path}\\n\")\n",
        "        print(f\"Created {output_path} with {len(file_list)} files.\")\n",
        "\n",
        "create_scp(train_files, \"train\")\n",
        "create_scp(val_files, \"val\")"
      ],
      "metadata": {
        "id": "QItPvTylKWXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77daa09-2322-4e81-b9e3-7eea9aa0a9f4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created /content/SepReformer/data/custom/train_s1.scp with 160 files.\n",
            "Created /content/SepReformer/data/custom/train_s2.scp with 160 files.\n",
            "Created /content/SepReformer/data/custom/train_mix_clean.scp with 160 files.\n",
            "Created /content/SepReformer/data/custom/val_s1.scp with 40 files.\n",
            "Created /content/SepReformer/data/custom/val_s2.scp with 40 files.\n",
            "Created /content/SepReformer/data/custom/val_mix_clean.scp with 40 files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install loguru torchinfo ptflops thop mir_eval"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Zd903Ld6t9SN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32977af-dc4e-4bc6-9ca9-adfeeba48114"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loguru\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.5-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting thop\n",
            "  Using cached thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting mir_eval\n",
            "  Downloading mir_eval-0.8.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from ptflops) (2.9.0+cu128)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.12/dist-packages (from mir_eval) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mir_eval) (1.16.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mir_eval) (4.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.3)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading ptflops-0.7.5-py3-none-any.whl (19 kB)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading mir_eval-0.8.2-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.8/102.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchinfo, loguru, mir_eval, thop, ptflops\n",
            "Successfully installed loguru-0.7.3 mir_eval-0.8.2 ptflops-0.7.5 thop-0.1.1.post2209072238 torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_content = \"\"\"\n",
        "project: \"[Project] SepReformer\"\n",
        "notes: \"Training on custom dataset - Fixed Scheduler\"\n",
        "\n",
        "config:\n",
        "    dataset:\n",
        "        max_len: 32000\n",
        "        sampling_rate: 8000\n",
        "        scp_dir: \"data/custom\"\n",
        "        train:\n",
        "            mixture: \"train_mix_clean.scp\"\n",
        "            spk1: \"train_s1.scp\"\n",
        "            spk2: \"train_s2.scp\"\n",
        "            dynamic_mixing: true\n",
        "        valid:\n",
        "            mixture: \"val_mix_clean.scp\"\n",
        "            spk1: \"val_s1.scp\"\n",
        "            spk2: \"val_s2.scp\"\n",
        "        test:\n",
        "            mixture: \"val_mix_clean.scp\"\n",
        "            spk1: \"val_s1.scp\"\n",
        "            spk2: \"val_s2.scp\"\n",
        "\n",
        "    dataloader:\n",
        "        batch_size: 1\n",
        "        pin_memory: true\n",
        "        num_workers: 4\n",
        "        drop_last: true\n",
        "\n",
        "    model:\n",
        "        num_stages: 4\n",
        "        num_spks: 2\n",
        "        module_audio_enc:\n",
        "            in_channels: 1\n",
        "            out_channels: 256\n",
        "            kernel_size: 16\n",
        "            stride: 4\n",
        "            groups: 1\n",
        "            bias: false\n",
        "        module_feature_projector:\n",
        "            num_channels: 256\n",
        "            in_channels: 256\n",
        "            out_channels: 128\n",
        "            kernel_size: 1\n",
        "            bias: false\n",
        "        module_separator:\n",
        "            num_stages: 4\n",
        "            relative_positional_encoding:\n",
        "                in_channels: 128\n",
        "                num_heads: 8\n",
        "                maxlen: 2000\n",
        "                embed_v: false\n",
        "            enc_stage:\n",
        "                global_blocks:\n",
        "                    in_channels: 128\n",
        "                    num_mha_heads: 8\n",
        "                    dropout_rate: 0.05\n",
        "                local_blocks:\n",
        "                    in_channels: 128\n",
        "                    kernel_size: 65\n",
        "                    dropout_rate: 0.05\n",
        "                down_conv_layer:\n",
        "                    in_channels: 128\n",
        "                    samp_kernel_size: 5\n",
        "            spk_split_stage:\n",
        "                in_channels: 128\n",
        "                num_spks: 2\n",
        "            simple_fusion:\n",
        "                out_channels: 128\n",
        "            dec_stage:\n",
        "                num_spks: 2\n",
        "                global_blocks:\n",
        "                    in_channels: 128\n",
        "                    num_mha_heads: 8\n",
        "                    dropout_rate: 0.05\n",
        "                local_blocks:\n",
        "                    in_channels: 128\n",
        "                    kernel_size: 65\n",
        "                    dropout_rate: 0.05\n",
        "                spk_attention:\n",
        "                    in_channels: 128\n",
        "                    num_mha_heads: 8\n",
        "                    dropout_rate: 0.05\n",
        "        module_output_layer:\n",
        "            in_channels: 256\n",
        "            out_channels: 128\n",
        "            num_spks: 2\n",
        "        module_audio_dec:\n",
        "            in_channels: 256\n",
        "            out_channels: 1\n",
        "            kernel_size: 16\n",
        "            stride: 4\n",
        "            bias: false\n",
        "\n",
        "    criterion:\n",
        "        name: [\"PIT_SISNR_mag\", \"PIT_SISNR_time\", \"PIT_SISNRi\", \"PIT_SDRi\"]\n",
        "        PIT_SISNR_mag:\n",
        "            frame_length: 512\n",
        "            frame_shift: 128\n",
        "            window: 'hann'\n",
        "            num_stages: 4\n",
        "            num_spks: 2\n",
        "            scale_inv: true\n",
        "            mel_opt: false\n",
        "        PIT_SISNR_time:\n",
        "            num_spks: 2\n",
        "            scale_inv: true\n",
        "        PIT_SISNRi:\n",
        "            num_spks: 2\n",
        "            scale_inv: true\n",
        "        PIT_SDRi:\n",
        "            dump: 0\n",
        "\n",
        "    optimizer:\n",
        "        name: [\"AdamW\"]\n",
        "        AdamW:\n",
        "            lr: 1.0e-3\n",
        "            weight_decay: 1.0e-2\n",
        "\n",
        "    scheduler:\n",
        "        # Restored \"WarmupConstantSchedule\" here\n",
        "        name: [\"ReduceLROnPlateau\", \"WarmupConstantSchedule\"]\n",
        "        ReduceLROnPlateau:\n",
        "            mode: \"min\"\n",
        "            min_lr: 1.0e-10\n",
        "            factor: 0.8\n",
        "            patience: 2\n",
        "        WarmupConstantSchedule:\n",
        "            warmup_steps: 100\n",
        "\n",
        "    check_computations:\n",
        "        dummy_len: 16000\n",
        "\n",
        "    engine:\n",
        "        max_epoch: 90\n",
        "        gpuid: \"0\"\n",
        "        mvn: false\n",
        "        clip_norm: 5\n",
        "        start_scheduling: 10\n",
        "        test_epochs: [50, 90]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"models/SepReformer_Base_WSJ0/configs.yaml\", \"w\") as f:\n",
        "    f.write(config_content)\n",
        "print(\"Config file updated with restored Scheduler settings!\")\n",
        "!rm -rf models/SepReformer_Base_WSJ0/log\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "import os\n",
        "\n",
        "# Path to the file\n",
        "engine_path = '/content/SepReformer/models/SepReformer_Base_WSJ0/engine.py'\n",
        "\n",
        "# Read the file\n",
        "with open(engine_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Fix the specific line\n",
        "new_lines = []\n",
        "found = False\n",
        "for line in lines:\n",
        "    # Look for the save_checkpoint_per_best call\n",
        "    if 'util_engine.save_checkpoint_per_best' in line:\n",
        "        # We need to remove the last argument (wandb_run or getattr...)\n",
        "        # The correct call should end after 'self.checkpoint_path'\n",
        "        # Let's construct the correct line based on the error message.\n",
        "        # It takes 7 args: valid_loss_best, val_loss, train_loss, epoch, model, optimizer, path\n",
        "\n",
        "        indent = line.split(\"valid_loss_best\")[0] # Preserve indentation\n",
        "        new_line = (\n",
        "            f\"{indent}valid_loss_best = util_engine.save_checkpoint_per_best(\"\n",
        "            f\"valid_loss_best, valid_loss_src_time, train_loss_src_time, epoch, \"\n",
        "            f\"self.model, self.main_optimizer, self.checkpoint_path)\\n\"\n",
        "        )\n",
        "        new_lines.append(new_line)\n",
        "        found = True\n",
        "        print(\"Fixed line to 7 arguments.\")\n",
        "    else:\n",
        "        new_lines.append(line)\n",
        "\n",
        "if found:\n",
        "    with open(engine_path, 'w') as f:\n",
        "        f.writelines(new_lines)\n",
        "    print(\"Successfully patched engine.py!\")\n",
        "else:\n",
        "    print(\"Could not find the specific line to patch. Please check file content.\")\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "import os\n",
        "\n",
        "# ×”× ×ª×™×‘ ×œ×§×•×‘×¥\n",
        "engine_path = '/content/SepReformer/models/SepReformer_Base_WSJ0/engine.py'\n",
        "\n",
        "# ×§×¨×™××ª ×”×§×•×‘×¥\n",
        "with open(engine_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# ×ª×™×§×•×Ÿ ×”×©×•×¨×”\n",
        "new_lines = []\n",
        "found = False\n",
        "for line in lines:\n",
        "    # ××—×¤×©×™× ××ª ×”×©×•×¨×” ×©×× ×¡×” ×œ×¨×©×•× ××ª ×”-Learning Rate\n",
        "    if 'writer_src.add_scalars(\"Learning Rate\"' in line:\n",
        "        # ××—×œ×™×¤×™× ××ª add_scalars ×‘-add_scalar (×‘×™×—×™×“)\n",
        "        new_line = line.replace('add_scalars', 'add_scalar')\n",
        "        new_lines.append(new_line)\n",
        "        found = True\n",
        "        print(\"Fixed line:\", new_line.strip())\n",
        "    else:\n",
        "        new_lines.append(line)\n",
        "\n",
        "if found:\n",
        "    with open(engine_path, 'w') as f:\n",
        "        f.writelines(new_lines)\n",
        "    print(\"Successfully patched engine.py (add_scalar fix)!\")\n",
        "else:\n",
        "    print(\"Could not find the specific line to patch.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myoGETBJynof",
        "outputId": "82d81b38-bd4d-485c-d3b9-d006af654039"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config file updated with restored Scheduler settings!\n",
            "Fixed line to 7 arguments.\n",
            "Successfully patched engine.py!\n",
            "Fixed line: writer_src.add_scalar(\"Learning Rate\", self.main_optimizer.param_groups[0]['lr'], epoch)\n",
            "Successfully patched engine.py (add_scalar fix)!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# control **epochs**"
      ],
      "metadata": {
        "id": "TWswwwKBtn14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "config_path = 'models/SepReformer_Base_WSJ0/configs.yaml'\n",
        "\n",
        "# ×§×¨×™××ª ×”×§×•×‘×¥\n",
        "with open(config_path, 'r') as f:\n",
        "    # × ×˜×¢×Ÿ ××ª ×”×§×•×‘×¥ ×›×˜×§×¡×˜ ×¤×©×•×˜ ×›×“×™ ×œ× ×œ×©×‘×•×¨ ××ª ×”×”×¢×¨×•×ª ×•×”××‘× ×” ×”××™×•×—×“\n",
        "    content = f.read()\n",
        "\n",
        "# ×‘×™×¦×•×¢ ×”×—×œ×¤×•×ª ×¤×©×•×˜×•×ª ×œ×©×™× ×•×™ ××¡×¤×¨ ×”-Epochs\n",
        "content = content.replace('max_epoch: 6', 'max_epoch: 90')\n",
        "content = content.replace('test_epochs: [6]', 'test_epochs: [90]')\n",
        "\n",
        "# ×©××™×¨×” ××—×“×©\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"Updated config: max_epoch = 6, test_epochs = [6]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaFTnm1Q38i4",
        "outputId": "d1384312-f8c8-47a7-ce46-97692a259be8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated config: max_epoch = 6, test_epochs = [6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ×”× ×ª×™×‘ ×œ×§×•×‘×¥ ×”×‘×¢×™×™×ª×™\n",
        "dataset_path = '/content/SepReformer/models/SepReformer_Base_WSJ0/dataset.py'\n",
        "\n",
        "# ×§×¨×™××ª ×”×§×•×‘×¥\n",
        "with open(dataset_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# ×ª×™×§×•×Ÿ ×”×œ×•×’×™×§×”\n",
        "new_lines = []\n",
        "found = False\n",
        "for line in lines:\n",
        "    # ××—×¤×©×™× ××ª ×”×©×•×¨×” ×©×× ×¡×” ×œ×¤×¨×§ ××ª ×©××•×ª ×”×§×‘×¦×™× ×¢× split('_')\n",
        "    # ×”×©×•×¨×” ×”××§×•×¨×™×ª × ×¨××™×ª ×‘×¢×¨×š ×›×š:\n",
        "    # tmp1 = key.split('_')[1][:3] != key_random.split('_')[3][:3]\n",
        "    if \"key.split('_')\" in line and \"key_random.split('_')\" in line:\n",
        "        # ××—×œ×™×¤×™× ×‘×œ×•×’×™×§×” ×¤×©×•×˜×” ×©×ª××™×“ ××—×–×™×¨×” True (×××¤×©×¨×ª ×¢×¨×‘×•×‘)\n",
        "        # ×©×•××¨×™× ×¢×œ ×”×”×–×—×” (Indentation) ×”××§×•×¨×™×ª\n",
        "        indent = line[:line.find(\"tmp1\")]\n",
        "        new_line = f\"{indent}tmp1 = True # Patched for custom filenames\\n\"\n",
        "        new_lines.append(new_line)\n",
        "\n",
        "        # ×× ×—× ×• ×¦×¨×™×›×™× ×’× ×œ× ×˜×¨×œ ××ª ×”×©×•×¨×•×ª ×”×‘××•×ª ×©×‘×•×“×§×•×ª ××ª tmp2 ×•××ª ×”×ª× ××™ ×”××•×¨×›×‘\n",
        "        # ××‘×œ ×”×“×¨×š ×”×›×™ ×§×œ×” ×”×™× ×œ×ª×ª ×œ-tmp1 ×œ×”×™×•×ª True, ×•×œ×©× ×•×ª ×’× ××ª tmp2 ×× ×¦×¨×™×š.\n",
        "        # ×œ××¢×©×”, ×”×¤×ª×¨×•×Ÿ ×”×›×™ ×™×¦×™×‘ ×”×•× ×œ×¢×˜×•×£ ××ª ×›×œ ×”×‘×œ×•×§ ×‘-try/except, ××‘×œ ×¢×¨×™×›×” ×›×–×• ××¡×•×‘×›×ª ×‘×¡×§×¨×™×¤×˜.\n",
        "        # ×”×¤×ª×¨×•×Ÿ ×”×¤×©×•×˜: × ×”×¤×•×š ××ª ×›×œ ×‘×“×™×§×•×ª ×”×“×•×‘×¨×™× ×œ-True.\n",
        "        found = True\n",
        "        print(\"Fixed tmp1 check.\")\n",
        "\n",
        "    elif \"tmp2 =\" in line and \"key.split\" in line:\n",
        "         indent = line[:line.find(\"tmp2\")]\n",
        "         new_lines.append(f\"{indent}tmp2 = True # Patched\\n\")\n",
        "         print(\"Fixed tmp2 check.\")\n",
        "\n",
        "    else:\n",
        "        new_lines.append(line)\n",
        "\n",
        "if found:\n",
        "    with open(dataset_path, 'w') as f:\n",
        "        f.writelines(new_lines)\n",
        "    print(\"âœ… Successfully patched dataset.py to support custom filenames!\")\n",
        "else:\n",
        "    print(\"âŒ Could not find the lines to patch. Please check file content manually.\")\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "file_path = '/content/SepReformer/models/SepReformer_Base_WSJ0/dataset.py'\n",
        "\n",
        "print(f\"ğŸ”§ Fixing {file_path}...\")\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "new_lines = []\n",
        "fixed = False\n",
        "\n",
        "for line in lines:\n",
        "    # ×–×™×”×•×™ ×”×©×•×¨×” ×”××©×•×‘×©×ª (×©××›×™×œ×” ×’× tmp2 ×•×’× tmp1 ×‘×™×—×“)\n",
        "    if \"tmp2 =\" in line and \"tmp1 =\" in line:\n",
        "        # ×©×•××¨×™× ×¢×œ ×”×”×–×—×” (Indentation) ×”××§×•×¨×™×ª\n",
        "        indent = line[:line.find(\"tmp2\")]\n",
        "        # ×›×•×ª×‘×™× ××ª ×”×¤×§×•×“×•×ª ×‘×©×•×¨×•×ª × ×¤×¨×“×•×ª\n",
        "        new_lines.append(f\"{indent}tmp1 = True # Fixed\\n\")\n",
        "        new_lines.append(f\"{indent}tmp2 = True # Fixed\\n\")\n",
        "        fixed = True\n",
        "        print(\"âœ… Found and fixed the broken line!\")\n",
        "\n",
        "    # ×˜×™×¤×•×œ ×‘××§×¨×™× ×©×‘×”× ×”×©×•×¨×•×ª ×¢×“×™×™×Ÿ ××§×•×¨×™×•×ª ××š ×œ× ××ª××™××•×ª (×œ×× ×™×¢×ª ×‘×¢×™×•×ª ×¢×ª×™×“×™×•×ª)\n",
        "    elif \"tmp1 =\" in line and \"key.split\" in line:\n",
        "        indent = line[:line.find(\"tmp1\")]\n",
        "        new_lines.append(f\"{indent}tmp1 = True # Patched\\n\")\n",
        "        fixed = True\n",
        "    elif \"tmp2 =\" in line and \"key.split\" in line:\n",
        "        indent = line[:line.find(\"tmp2\")]\n",
        "        new_lines.append(f\"{indent}tmp2 = True # Patched\\n\")\n",
        "        fixed = True\n",
        "\n",
        "    else:\n",
        "        new_lines.append(line)\n",
        "\n",
        "if fixed:\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.writelines(new_lines)\n",
        "    print(\"ğŸš€ File updated successfully.\")\n",
        "else:\n",
        "    print(\"âš ï¸ No broken lines found. The file might be already fixed or different than expected.\")\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# ×”× ×ª×™×‘ ×œ×§×•×‘×¥ ×”×‘×¢×™×™×ª×™\n",
        "dataset_path = '/content/SepReformer/models/SepReformer_Base_WSJ0/dataset.py'\n",
        "\n",
        "# ×§×¨×™××ª ×”×§×•×‘×¥\n",
        "with open(dataset_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# ×ª×™×§×•×Ÿ ×”×©×•×¨×”\n",
        "new_lines = []\n",
        "found = False\n",
        "for line in lines:\n",
        "    # ××—×¤×©×™× ××ª ×”×©×•×¨×” ×©××‘×¦×¢×ª speed_aug\n",
        "    if 'self.speed_aug(torch.tensor(samps_tmp))' in line:\n",
        "        # ×¤×©×•×˜ × ×“×œ×’ ×¢×œ×™×” ××• × ×”×¤×•×š ××•×ª×” ×œ×”×¢×¨×”.\n",
        "        # ×‘××§×•× ×œ××—×•×§, × ×©××™×¨ ××ª samps_tmp ×›××• ×©×”×•×.\n",
        "        # ×”×©×•×¨×” ×”××§×•×¨×™×ª: samps_tmp = np.array(self.speed_aug(torch.tensor(samps_tmp))[0])\n",
        "        # ×”×—×“×©×”: # samps_tmp = ... (skipped)\n",
        "        new_lines.append(f\"            # {line.strip()} # PATCHED: Disabled speed_aug\\n\")\n",
        "        found = True\n",
        "        print(\"Disabled speed_aug line.\")\n",
        "    else:\n",
        "        new_lines.append(line)\n",
        "\n",
        "if found:\n",
        "    with open(dataset_path, 'w') as f:\n",
        "        f.writelines(new_lines)\n",
        "    print(\"âœ… Successfully patched dataset.py to remove speed augmentation bug!\")\n",
        "else:\n",
        "    print(\"âš ï¸ Could not find the speed_aug line. It might be already fixed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK6KafFeBPzL",
        "outputId": "6c4b863d-068c-4bc3-da42-0f8c0aad3a9a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed tmp1 check.\n",
            "Fixed tmp1 check.\n",
            "âœ… Successfully patched dataset.py to support custom filenames!\n",
            "ğŸ”§ Fixing /content/SepReformer/models/SepReformer_Base_WSJ0/dataset.py...\n",
            "âœ… Found and fixed the broken line!\n",
            "ğŸš€ File updated successfully.\n",
            "Disabled speed_aug line.\n",
            "âœ… Successfully patched dataset.py to remove speed augmentation bug!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL: AttentionSpeakerSplit â€” Drop-in Replacement\n",
        "# ============================================================\n",
        "# Run ONCE after the repo is cloned (and before training).\n",
        "# It will:\n",
        "#   1. Inject AttentionSpeakerSplit into modules/module.py\n",
        "#   2. Replace every SpkSplitStage() call with it\n",
        "#   3. Run a shape + gradient smoke-test\n",
        "# ============================================================\n",
        "\n",
        "import os, textwrap\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1.  Class definition (injected as text into module.py)\n",
        "# ----------------------------------------------------------\n",
        "NEW_CLASS = textwrap.dedent(\"\"\"\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "#  AttentionSpeakerSplit  (drop-in replacement for SpkSplitStage)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class AttentionSpeakerSplit(nn.Module):\n",
        "    \\\"\\\"\\\"\n",
        "    Dynamic, cross-attention-based speaker split.\n",
        "\n",
        "    Architecture\n",
        "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    â€¢ num_spks learnable prototype vectors (speaker_queries) act as Queries Q.\n",
        "    â€¢ The shared encoder output x provides Keys K and Values V.\n",
        "    â€¢ Cross-attention distils a global \"speaker context\" from the mixture â€”\n",
        "      each query learns what its assigned speaker sounds like across the corpus.\n",
        "    â€¢ That context modulates the input frame-by-frame via a sigmoid gate:\n",
        "          out[s, t] = x[t] * sigmoid(ctx[s]) + proj(x[t])   for each speaker s\n",
        "    â€¢ A per-speaker GLU feed-forward sharpens the gated output.\n",
        "    â€¢ Residual connections stabilise training from random initialisation.\n",
        "\n",
        "    Shapes\n",
        "    â”€â”€â”€â”€â”€â”€â”€\n",
        "    Input  x : (batch, dim, time)\n",
        "    Output   : (batch, num_spks, dim, time)  â† feeds simple_fusion unchanged\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    def __init__(self, in_channels: int, num_spks: int,\n",
        "                 num_heads: int = 8, dropout: float = 0.05):\n",
        "        super().__init__()\n",
        "        assert in_channels % num_heads == 0, (\n",
        "            f\"in_channels ({in_channels}) must be divisible by \"\n",
        "            f\"num_heads ({num_heads})\"\n",
        "        )\n",
        "        self.in_channels = in_channels\n",
        "        self.num_spks    = num_spks\n",
        "\n",
        "        # â”€â”€ Learnable speaker prototype queries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # (num_spks, in_channels): one row per speaker.\n",
        "        # These are the only globally shared parameters encoding speaker identity.\n",
        "        self.speaker_queries = nn.Parameter(\n",
        "            torch.empty(num_spks, in_channels)\n",
        "        )\n",
        "        nn.init.trunc_normal_(self.speaker_queries, std=0.02)\n",
        "\n",
        "        # â”€â”€ Cross-attention (Q = prototypes, K/V = mixture features) â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "            embed_dim   = in_channels,\n",
        "            num_heads   = num_heads,\n",
        "            dropout     = dropout,\n",
        "            batch_first = False,   # expects (seq, batch, dim)\n",
        "        )\n",
        "        self.attn_norm = nn.LayerNorm(in_channels)\n",
        "\n",
        "        # â”€â”€ Per-speaker gated feed-forward (GLU) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # Sharpens the modulated representation independently per speaker.\n",
        "        # Linear: dim â†’ 2*dim, then GLU halves back to dim.\n",
        "        self.ff_gate = nn.Sequential(\n",
        "            nn.LayerNorm(in_channels),\n",
        "            nn.Linear(in_channels, in_channels * 2, bias=True),\n",
        "            nn.GLU(dim=-1),\n",
        "        )\n",
        "        self.ff_norm = nn.LayerNorm(in_channels)\n",
        "\n",
        "        # â”€â”€ Residual input projection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        self.res_proj = nn.Linear(in_channels, in_channels, bias=False)\n",
        "\n",
        "        # â”€â”€ Output normalisation (GroupNorm matches rest of model) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        self.out_norm = nn.GroupNorm(1, in_channels)\n",
        "\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \\\"\\\"\\\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : (batch, dim, time)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out : (batch, num_spks, dim, time)\n",
        "        \\\"\\\"\\\"\n",
        "        B, D, T = x.shape\n",
        "\n",
        "        # â”€â”€ Step 1: Cross-Attention to get per-speaker global context â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # MultiheadAttention: (seq, batch, dim)\n",
        "        kv = x.permute(2, 0, 1)                            # (T, B, D)\n",
        "\n",
        "        # Expand speaker_queries to batch: (num_spks, B, D)\n",
        "        q  = self.speaker_queries.unsqueeze(1).expand(-1, B, -1)\n",
        "\n",
        "        # Each speaker query attends over all time steps.\n",
        "        attn_out, _ = self.cross_attn(\n",
        "            query        = q,\n",
        "            key          = kv,\n",
        "            value        = kv,\n",
        "            need_weights = False,           # skip weight materialisation\n",
        "        )\n",
        "        # attn_out: (num_spks, B, D)\n",
        "\n",
        "        # Prototype queries as residual, then LayerNorm.\n",
        "        attn_out = self.attn_norm(attn_out + q)             # (num_spks, B, D)\n",
        "\n",
        "        # â”€â”€ Step 2: Frame-wise sigmoid gate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # Expand input over speakers\n",
        "        x_exp   = x.unsqueeze(1).expand(-1, self.num_spks, -1, -1)\n",
        "        #         (B, 1, D, T) â†’ (B, S, D, T)\n",
        "\n",
        "        # attn_out: (S, B, D) â†’ (B, S, D, 1)  â€” broadcast over time\n",
        "        spk_ctx = attn_out.permute(1, 0, 2).unsqueeze(-1)  # (B, S, D, 1)\n",
        "\n",
        "        # Residual baseline: project input, broadcast over speakers\n",
        "        res = self.res_proj(x.permute(0, 2, 1))            # (B, T, D)\n",
        "        res = res.permute(0, 2, 1).unsqueeze(1)            # (B, 1, D, T)\n",
        "\n",
        "        # Gate + residual\n",
        "        out = x_exp * torch.sigmoid(spk_ctx) + res         # (B, S, D, T)\n",
        "\n",
        "        # â”€â”€ Step 3: Per-speaker GLU feed-forward â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # Reshape to (B*S, T, D) for the FF layers\n",
        "        out_flat = out.reshape(B * self.num_spks, D, T).permute(0, 2, 1)\n",
        "        #          (B*S, T, D)\n",
        "        ff_out   = self.ff_gate(out_flat)                   # (B*S, T, D)\n",
        "        out_flat = self.ff_norm(out_flat + ff_out)          # residual\n",
        "        out      = out_flat.permute(0, 2, 1).reshape(B, self.num_spks, D, T)\n",
        "\n",
        "        # â”€â”€ Step 4: GroupNorm over channel dim (per speaker) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        out = self.out_norm(\n",
        "            out.reshape(B * self.num_spks, D, T)\n",
        "        ).reshape(B, self.num_spks, D, T)\n",
        "\n",
        "        return out                                          # (B, S, D, T)\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2.  Patch helpers\n",
        "# ----------------------------------------------------------\n",
        "def patch_module_py(path: str) -> None:\n",
        "    \"\"\"Inject class + swap SpkSplitStage calls in modules/module.py.\"\"\"\n",
        "    if not os.path.isfile(path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"module.py not found at: {path}\\n\"\n",
        "            \"Make sure the repo is cloned before running this cell.\"\n",
        "        )\n",
        "    with open(path, \"r\") as f:\n",
        "        src = f.read()\n",
        "\n",
        "    # Guard\n",
        "    if \"AttentionSpeakerSplit\" in src:\n",
        "        print(\"âš¡ module.py already contains AttentionSpeakerSplit â€” skipping inject.\")\n",
        "    else:\n",
        "        marker = \"class SpkSplitStage\"\n",
        "        if marker not in src:\n",
        "            raise RuntimeError(\n",
        "                \"Could not find 'class SpkSplitStage' in module.py.\\n\"\n",
        "                \"Search for the speaker-split class and insert \"\n",
        "                \"AttentionSpeakerSplit above it manually.\"\n",
        "            )\n",
        "        pos = src.index(marker)\n",
        "        src = src[:pos] + NEW_CLASS + src[pos:]\n",
        "        print(\"âœ…  AttentionSpeakerSplit class injected into module.py.\")\n",
        "\n",
        "    # Swap instantiation calls\n",
        "    before = src\n",
        "    src    = src.replace(\"SpkSplitStage(\", \"AttentionSpeakerSplit(\")\n",
        "    n_swapped = before.count(\"SpkSplitStage(\")\n",
        "    if src != before:\n",
        "        print(f\"âœ…  Swapped {n_swapped} SpkSplitStage( â†’ AttentionSpeakerSplit( in module.py.\")\n",
        "    else:\n",
        "        print(\"  â„¹ï¸  No SpkSplitStage( call found in module.py (may be in model.py).\")\n",
        "\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(src)\n",
        "    print(\"ğŸ’¾  module.py saved.\")\n",
        "\n",
        "\n",
        "def patch_other(path: str) -> None:\n",
        "    \"\"\"Swap SpkSplitStage calls in any additional file (model.py etc.).\"\"\"\n",
        "    if not os.path.isfile(path):\n",
        "        return\n",
        "    fname = os.path.basename(path)\n",
        "    with open(path, \"r\") as f:\n",
        "        src = f.read()\n",
        "    if \"AttentionSpeakerSplit\" in src:\n",
        "        print(f\"âš¡ {fname}: already patched â€” skipping.\")\n",
        "        return\n",
        "    before = src\n",
        "    src    = src.replace(\"SpkSplitStage(\", \"AttentionSpeakerSplit(\")\n",
        "    if src == before:\n",
        "        print(f\"  â„¹ï¸  {fname}: no SpkSplitStage( calls â€” nothing to do.\")\n",
        "        return\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(src)\n",
        "    n = before.count(\"SpkSplitStage(\")\n",
        "    print(f\"âœ…  {fname}: swapped {n} call(s).\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3.  Run patches\n",
        "# ----------------------------------------------------------\n",
        "BASE = \"/content/SepReformer/models/SepReformer_Base_WSJ0\"\n",
        "\n",
        "patch_module_py(os.path.join(BASE, \"modules\", \"module.py\"))\n",
        "patch_other(os.path.join(BASE, \"model.py\"))\n",
        "patch_other(os.path.join(BASE, \"engine.py\"))\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4.  Smoke-test (runs the class in-process, no full model needed)\n",
        "# ----------------------------------------------------------\n",
        "print(\"\\nâ”€â”€â”€ Shape + Gradient Smoke Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
        "\n",
        "exec(compile(NEW_CLASS, \"<string>\", \"exec\"), globals())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"  device: {device}\")\n",
        "\n",
        "# Match configs.yaml: in_channels=128, num_spks=2\n",
        "BATCH, DIM, TIME, NUM_SPKS = 2, 128, 200, 2\n",
        "\n",
        "module = AttentionSpeakerSplit(\n",
        "    in_channels=DIM, num_spks=NUM_SPKS,\n",
        "    num_heads=8, dropout=0.05\n",
        ").to(device)\n",
        "\n",
        "dummy = torch.randn(BATCH, DIM, TIME, device=device)\n",
        "\n",
        "# Forward shape\n",
        "with torch.no_grad():\n",
        "    out = module(dummy)\n",
        "assert out.shape == (BATCH, NUM_SPKS, DIM, TIME), \\\n",
        "    f\"Shape mismatch: expected {(BATCH, NUM_SPKS, DIM, TIME)}, got {tuple(out.shape)}\"\n",
        "print(f\"  Input  shape : {tuple(dummy.shape)}\")\n",
        "print(f\"  Output shape : {tuple(out.shape)}  âœ…\")\n",
        "\n",
        "# Gradient flow\n",
        "module(dummy).sum().backward()\n",
        "assert module.speaker_queries.grad is not None\n",
        "print(f\"  â€–âˆ‡ speaker_queriesâ€– = {module.speaker_queries.grad.norm():.4f}  âœ…  gradients flow\")\n",
        "\n",
        "# Speaker outputs should differ from each other\n",
        "diff = (out[:, 0] - out[:, 1]).abs().mean().item()\n",
        "print(f\"  Speaker output mean |diff| = {diff:.5f}  \" \\\n",
        "      f\"{'âœ… differentiated' if diff > 1e-6 else 'âš ï¸ identical â€” check queries init'}\")\n",
        "\n",
        "n_p = sum(p.numel() for p in module.parameters())\n",
        "print(f\"  Module params  : {n_p:,}  ({n_p/1e6:.3f} M)\")\n",
        "\n",
        "print(\"\"\"\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ğŸ‰  Patch complete!\n",
        "\n",
        "If SepReformer modules were already imported this session,\n",
        "restart the runtime now. Then run training as normal:\n",
        "\n",
        "    !python run.py --model SepReformer_Base_WSJ0 --engine-mode train\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\"\"\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 5.  Post-patch Indentation Fix for module.py\n",
        "# ----------------------------------------------------------\n",
        "module_file_path = os.path.join(BASE, \"modules\", \"module.py\")\n",
        "\n",
        "if os.path.exists(module_file_path):\n",
        "    print(\"\\nâ”€â”€â”€ Fixing indentation in module.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
        "    with open(module_file_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    fixed_lines = []\n",
        "    for line in lines:\n",
        "        stripped_line = line.lstrip()\n",
        "        # If a line starts with 'class' or 'def' (and is not commented out),\n",
        "        # ensure it has no leading whitespace. This assumes these are top-level constructs.\n",
        "        if (stripped_line.startswith(\"class \") or stripped_line.startswith(\"def \")) and \\\n",
        "           not stripped_line.startswith(\"#\"):\n",
        "            fixed_lines.append(stripped_line) # Remove all leading whitespace\n",
        "        else:\n",
        "            fixed_lines.append(line) # Keep original indentation\n",
        "\n",
        "    with open(module_file_path, \"w\") as f:\n",
        "        f.writelines(fixed_lines)\n",
        "    print(\"âœ…  Indentation in module.py adjusted. (Removed leading whitespace from top-level class/def statements)\")\n",
        "else:\n",
        "    print(f\"âš ï¸  module.py not found at {module_file_path}, skipping indentation fix.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgX6pqQu76uf",
        "outputId": "59684365-d59a-406f-dd3e-d86092bc3aa5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ…  AttentionSpeakerSplit class injected into module.py.\n",
            "âœ…  Swapped 2 SpkSplitStage( â†’ AttentionSpeakerSplit( in module.py.\n",
            "ğŸ’¾  module.py saved.\n",
            "  â„¹ï¸  model.py: no SpkSplitStage( calls â€” nothing to do.\n",
            "  â„¹ï¸  engine.py: no SpkSplitStage( calls â€” nothing to do.\n",
            "\n",
            "â”€â”€â”€ Shape + Gradient Smoke Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  device: cuda\n",
            "  Input  shape : (2, 128, 200)\n",
            "  Output shape : (2, 2, 128, 200)  âœ…\n",
            "  â€–âˆ‡ speaker_queriesâ€– = 0.0000  âœ…  gradients flow\n",
            "  Speaker output mean |diff| = 0.15260  âœ… differentiated\n",
            "  Module params  : 116,736  (0.117 M)\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ‰  Patch complete!\n",
            "\n",
            "If SepReformer modules were already imported this session,\n",
            "restart the runtime now. Then run training as normal:\n",
            "\n",
            "    !python run.py --model SepReformer_Base_WSJ0 --engine-mode train\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "\n",
            "â”€â”€â”€ Fixing indentation in module.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "âœ…  Indentation in module.py adjusted. (Removed leading whitespace from top-level class/def statements)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train**"
      ],
      "metadata": {
        "id": "z1AJZFXV1LIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import textwrap\n",
        "import torch\n",
        "\n",
        "# 0. Set working directory\n",
        "%cd /content/SepReformer\n",
        "\n",
        "# 1. Revert module.py to clean state\n",
        "# This ensures we don't have broken indentation from previous attempts\n",
        "print(\"ğŸ”„ Reverting files to clean state...\")\n",
        "!git checkout models/SepReformer_Base_WSJ0/modules/module.py\n",
        "!git checkout models/SepReformer_Base_WSJ0/model.py\n",
        "!git checkout models/SepReformer_Base_WSJ0/engine.py\n",
        "\n",
        "# 2. Correctly Re-Patch module.py\n",
        "module_path = 'models/SepReformer_Base_WSJ0/modules/module.py'\n",
        "\n",
        "# Using torch.nn explicitly since module.py might not have 'import torch.nn as nn'\n",
        "# FIX 1: using positional arguments for cross_attn to satisfy ptflops\n",
        "# FIX 2: Return (B*S, D, T) to match decoder expectation\n",
        "NEW_CLASS_CODE = textwrap.dedent(\"\"\"\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "#  AttentionSpeakerSplit  (drop-in replacement for SpkSplitStage)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class AttentionSpeakerSplit(torch.nn.Module):\n",
        "    def __init__(self, in_channels: int, num_spks: int,\n",
        "                 num_heads: int = 8, dropout: float = 0.05):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.num_spks    = num_spks\n",
        "        self.speaker_queries = torch.nn.Parameter(torch.empty(num_spks, in_channels))\n",
        "        torch.nn.init.trunc_normal_(self.speaker_queries, std=0.02)\n",
        "        self.cross_attn = torch.nn.MultiheadAttention(\n",
        "            embed_dim=in_channels, num_heads=num_heads, dropout=dropout, batch_first=False\n",
        "        )\n",
        "        self.attn_norm = torch.nn.LayerNorm(in_channels)\n",
        "        self.ff_gate = torch.nn.Sequential(\n",
        "            torch.nn.LayerNorm(in_channels),\n",
        "            torch.nn.Linear(in_channels, in_channels * 2, bias=True),\n",
        "            torch.nn.GLU(dim=-1),\n",
        "        )\n",
        "        self.ff_norm = torch.nn.LayerNorm(in_channels)\n",
        "        self.res_proj = torch.nn.Linear(in_channels, in_channels, bias=False)\n",
        "        self.out_norm = torch.nn.GroupNorm(1, in_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, D, T = x.shape\n",
        "        kv = x.permute(2, 0, 1)\n",
        "        q  = self.speaker_queries.unsqueeze(1).expand(-1, B, -1)\n",
        "\n",
        "        # PTFLOPS FIX: Pass q, k, v as positional arguments.\n",
        "        # ptflops hook expects to unpack (q, k, v) from input tuple.\n",
        "        attn_out, _ = self.cross_attn(q, kv, kv, need_weights=False)\n",
        "\n",
        "        attn_out = self.attn_norm(attn_out + q)\n",
        "        x_exp   = x.unsqueeze(1).expand(-1, self.num_spks, -1, -1)\n",
        "        spk_ctx = attn_out.permute(1, 0, 2).unsqueeze(-1)\n",
        "        res = self.res_proj(x.permute(0, 2, 1))\n",
        "        res = res.permute(0, 2, 1).unsqueeze(1)\n",
        "        out = x_exp * torch.sigmoid(spk_ctx) + res\n",
        "        out_flat = out.reshape(B * self.num_spks, D, T).permute(0, 2, 1)\n",
        "        ff_out   = self.ff_gate(out_flat)\n",
        "        out_flat = self.ff_norm(out_flat + ff_out)\n",
        "        out      = out_flat.permute(0, 2, 1).reshape(B, self.num_spks, D, T)\n",
        "\n",
        "        # Reshape to (B*S, D, T) for GroupNorm and final output\n",
        "        # Decoder expects flattened batch of speakers\n",
        "        out = self.out_norm(out.reshape(B * self.num_spks, D, T))\n",
        "        return out\n",
        "\"\"\")\n",
        "\n",
        "if os.path.exists(module_path):\n",
        "    with open(module_path, 'r') as f:\n",
        "        src = f.read()\n",
        "\n",
        "    if \"AttentionSpeakerSplit\" not in src:\n",
        "        # A. Rename the original class definition to avoid conflict\n",
        "        src = src.replace(\"class SpkSplitStage\", \"class _OldSpkSplitStage\")\n",
        "\n",
        "        # B. Replace all instantiation calls SpkSplitStage(...) -> AttentionSpeakerSplit(...)\n",
        "        src = src.replace(\"SpkSplitStage(\", \"AttentionSpeakerSplit(\")\n",
        "\n",
        "        # C. Append the new class to the END of the file\n",
        "        src += NEW_CLASS_CODE\n",
        "\n",
        "        with open(module_path, 'w') as f:\n",
        "            f.write(src)\n",
        "        print(\"âœ… module.py patched successfully.\")\n",
        "    else:\n",
        "        print(\"â„¹ï¸ module.py already patched.\")\n",
        "\n",
        "    # 3. Patch model.py and engine.py (Update usages)\n",
        "    other_files = [\n",
        "        'models/SepReformer_Base_WSJ0/model.py',\n",
        "        'models/SepReformer_Base_WSJ0/engine.py'\n",
        "    ]\n",
        "\n",
        "    for fpath in other_files:\n",
        "        if os.path.exists(fpath):\n",
        "            with open(fpath, 'r') as f:\n",
        "                src = f.read()\n",
        "\n",
        "            if \"SpkSplitStage(\" in src:\n",
        "                src = src.replace(\"SpkSplitStage(\", \"AttentionSpeakerSplit(\")\n",
        "                with open(fpath, 'w') as f:\n",
        "                    f.write(src)\n",
        "                print(f\"âœ… {os.path.basename(fpath)} updated.\")\n",
        "            else:\n",
        "                print(f\"â„¹ï¸ No usages found in {os.path.basename(fpath)}.\")\n",
        "\n",
        "else:\n",
        "    print(f\"âŒ Error: {module_path} not found.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKmP1wIUv8ux",
        "outputId": "345b2d28-e82d-4ecc-813d-470f7fe8cf08",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SepReformer\n",
            "ğŸ”„ Reverting files to clean state...\n",
            "Updated 1 path from the index\n",
            "Updated 0 paths from the index\n",
            "Updated 1 path from the index\n",
            "âœ… module.py patched successfully.\n",
            "â„¹ï¸ No usages found in model.py.\n",
            "â„¹ï¸ No usages found in engine.py.\n",
            "ğŸš€ Starting Training...\n",
            "2026-02-18 12:22:41.638806: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-02-18 12:22:41.657090: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771417361.679636    2620 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771417361.687003    2620 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771417361.706241    2620 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771417361.706275    2620 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771417361.706278    2620 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771417361.706281    2620 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-18 12:22:41.711130: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m2026-02-18 12:22:47.337\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mEntering 'main' (args=(Namespace(model='SepReformer_Base_WSJ0', engine_mode='train', sample_file=None, out_wav_dir=None),), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.338\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m20\u001b[0m - \u001b[34m\u001b[1mEntering 'parse_yaml' (args=('/content/SepReformer/models/SepReformer_Base_WSJ0/configs.yaml',), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.643\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m20\u001b[0m - \u001b[34m\u001b[1mExiting 'parse_yaml' (result={'project': '[Project] SepReformer', 'notes': 'Training on custom dataset - Fixed Scheduler', 'config': {'dataset': {'max_len': 32000, 'sampling_rate': 8000, 'scp_dir': 'data/custom', 'train': {'mixture': 'train_mix_clean.scp', 'spk1': 'train_s1.scp', 'spk2': 'train_s2.scp', 'dynamic_mixing': True}, 'valid': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}, 'test': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}}, 'dataloader': {'batch_size': 1, 'pin_memory': True, 'num_workers': 4, 'drop_last': True}, 'model': {'num_stages': 4, 'num_spks': 2, 'module_audio_enc': {'in_channels': 1, 'out_channels': 256, 'kernel_size': 16, 'stride': 4, 'groups': 1, 'bias': False}, 'module_feature_projector': {'num_channels': 256, 'in_channels': 256, 'out_channels': 128, 'kernel_size': 1, 'bias': False}, 'module_separator': {'num_stages': 4, 'relative_positional_encoding': {'in_channels': 128, 'num_heads': 8, 'maxlen': 2000, 'embed_v': False}, 'enc_stage': {'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'down_conv_layer': {'in_channels': 128, 'samp_kernel_size': 5}}, 'spk_split_stage': {'in_channels': 128, 'num_spks': 2}, 'simple_fusion': {'out_channels': 128}, 'dec_stage': {'num_spks': 2, 'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'spk_attention': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}}}, 'module_output_layer': {'in_channels': 256, 'out_channels': 128, 'num_spks': 2}, 'module_audio_dec': {'in_channels': 256, 'out_channels': 1, 'kernel_size': 16, 'stride': 4, 'bias': False}}, 'criterion': {'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, 'optimizer': {'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, 'scheduler': {'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, 'check_computations': {'dummy_len': 16000}, 'engine': {'max_epoch': 90, 'gpuid': '0', 'mvn': False, 'clip_norm': 5, 'start_scheduling': 10, 'test_epochs': [50, 90]}}})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.643\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m26\u001b[0m - \u001b[34m\u001b[1mEntering 'get_dataloaders' (args=(Namespace(model='SepReformer_Base_WSJ0', engine_mode='train', sample_file=None, out_wav_dir=None), {'max_len': 32000, 'sampling_rate': 8000, 'scp_dir': 'data/custom', 'train': {'mixture': 'train_mix_clean.scp', 'spk1': 'train_s1.scp', 'spk2': 'train_s2.scp', 'dynamic_mixing': True}, 'valid': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}, 'test': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}}, {'batch_size': 1, 'pin_memory': True, 'num_workers': 4, 'drop_last': True}), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.643\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mEntering 'MyDataset' (args=(), kwargs={'max_len': 32000, 'fs': 8000, 'partition': 'train', 'wave_scp_srcs': ['data/custom/train_s1.scp', 'data/custom/train_s2.scp'], 'wave_scp_mix': 'data/custom/train_mix_clean.scp', 'dynamic_mixing': True})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mCreate MyDataset for data/custom/train_mix_clean.scp with 160 utterances\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.644\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mExiting 'MyDataset' (result=<models.SepReformer_Base_WSJ0.dataset.MyDataset object at 0x7d391c5c8e60>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.645\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mEntering 'MyDataset' (args=(), kwargs={'max_len': 32000, 'fs': 8000, 'partition': 'valid', 'wave_scp_srcs': ['data/custom/val_s1.scp', 'data/custom/val_s2.scp'], 'wave_scp_mix': 'data/custom/val_mix_clean.scp', 'dynamic_mixing': False})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mCreate MyDataset for data/custom/val_mix_clean.scp with 40 utterances\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.645\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mExiting 'MyDataset' (result=<models.SepReformer_Base_WSJ0.dataset.MyDataset object at 0x7d3a1e9eb5c0>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.645\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mEntering 'MyDataset' (args=(), kwargs={'max_len': 32000, 'fs': 8000, 'partition': 'test', 'wave_scp_srcs': ['data/custom/val_s1.scp', 'data/custom/val_s2.scp'], 'wave_scp_mix': 'data/custom/val_mix_clean.scp', 'dynamic_mixing': False})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mCreate MyDataset for data/custom/val_mix_clean.scp with 40 utterances\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mExiting 'MyDataset' (result=<models.SepReformer_Base_WSJ0.dataset.MyDataset object at 0x7d3b5e9ad820>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m26\u001b[0m - \u001b[34m\u001b[1mExiting 'get_dataloaders' (result={'train': <torch.utils.data.dataloader.DataLoader object at 0x7d391ca49a30>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7d391c5fdd30>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7d391ca0f260>})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mEntering 'Model' (args=(), kwargs={'num_stages': 4, 'num_spks': 2, 'module_audio_enc': {'in_channels': 1, 'out_channels': 256, 'kernel_size': 16, 'stride': 4, 'groups': 1, 'bias': False}, 'module_feature_projector': {'num_channels': 256, 'in_channels': 256, 'out_channels': 128, 'kernel_size': 1, 'bias': False}, 'module_separator': {'num_stages': 4, 'relative_positional_encoding': {'in_channels': 128, 'num_heads': 8, 'maxlen': 2000, 'embed_v': False}, 'enc_stage': {'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'down_conv_layer': {'in_channels': 128, 'samp_kernel_size': 5}}, 'spk_split_stage': {'in_channels': 128, 'num_spks': 2}, 'simple_fusion': {'out_channels': 128}, 'dec_stage': {'num_spks': 2, 'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'spk_attention': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}}}, 'module_output_layer': {'in_channels': 256, 'out_channels': 128, 'num_spks': 2}, 'module_audio_dec': {'in_channels': 256, 'out_channels': 1, 'kernel_size': 16, 'stride': 4, 'bias': False}})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.817\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mExiting 'Model' (result=Model(\n",
            "  (audio_encoder): AudioEncoder(\n",
            "    (conv1d): Conv1d(1, 256, kernel_size=(16,), stride=(4,), bias=False)\n",
            "    (gelu): GELU(approximate='none')\n",
            "  )\n",
            "  (feature_projector): FeatureProjector(\n",
            "    (norm): GroupNorm(1, 256, eps=1e-08, affine=True)\n",
            "    (conv1d): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "  )\n",
            "  (separator): Separator(\n",
            "    (pos_emb): RelativePositionalEncoding(\n",
            "      (pe_k): Embedding(4000, 16)\n",
            "    )\n",
            "    (enc_stages): ModuleList(\n",
            "      (0-3): 4 x SepEncStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downconv): DownConvLayer(\n",
            "          (down_conv): Conv1d(128, 128, kernel_size=(5,), stride=(2,), padding=(2,), groups=128)\n",
            "          (BN): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (gelu): GELU(approximate='none')\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (bottleneck_G): SepEncStage(\n",
            "      (g_block_1): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_1): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (g_block_2): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_2): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (spk_split_block): AttentionSpeakerSplit(\n",
            "      (cross_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff_gate): Sequential(\n",
            "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (1): Linear(in_features=128, out_features=256, bias=True)\n",
            "        (2): GLU(dim=-1)\n",
            "      )\n",
            "      (ff_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (res_proj): Linear(in_features=128, out_features=128, bias=False)\n",
            "      (out_norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "    )\n",
            "    (simple_fusion): ModuleList(\n",
            "      (0-3): 4 x Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "    (dec_stages): ModuleList(\n",
            "      (0-3): 4 x SepDecStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_1): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_2): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_3): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_3): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_3): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (out_layer): OutputLayer(\n",
            "    (spe_block): Masking(\n",
            "      (gate_act): ReLU()\n",
            "    )\n",
            "    (end_conv1x1): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (1): GLU(dim=-1)\n",
            "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (audio_decoder): AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  (out_layer_bn): ModuleList(\n",
            "    (0-3): 4 x OutputLayer(\n",
            "      (spe_block): Masking(\n",
            "        (gate_act): ReLU()\n",
            "      )\n",
            "      (end_conv1x1): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): GLU(dim=-1)\n",
            "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder_bn): ModuleList(\n",
            "    (0-3): 4 x AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  )\n",
            "))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.908\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mEntering 'CriterionFactory' (args=({'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, device(type='cuda', index=0)), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:47.909\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mExiting 'CriterionFactory' (result=CriterionFactory(config={'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, device=device(type='cuda', index=0)))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SISNR_mag instance with args: {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.343\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SISNR_mag' (args=(device(type='cuda', index=0),), kwargs={'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.343\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.540\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.861\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.863\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.866\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.867\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.870\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.871\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.874\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SISNR_mag' (result=<PIT_SISNR_mag(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', num_stages=4, num_spks=2, scale_inv=True, mel_opt=False, stft = [STFT instance for 4 layers], mel_fb=Identity)>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SISNR_time instance with args: {'num_spks': 2, 'scale_inv': True}\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.874\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SISNR_time' (args=(device(type='cuda', index=0),), kwargs={'num_spks': 2, 'scale_inv': True})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.874\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SISNR_time' (result=<PIT_SISNR_time(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SISNRi instance with args: {'num_spks': 2, 'scale_inv': True}\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.874\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SISNRi' (args=(device(type='cuda', index=0),), kwargs={'num_spks': 2, 'scale_inv': True})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.874\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SISNRi' (result=<PIT_SISNRi(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SDRi instance with args: {'dump': 0}\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.875\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SDRi' (args=(device(type='cuda', index=0),), kwargs={'dump': 0})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.875\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SDRi' (result=<PIT_SDRi(device=device(type='cuda', index=0), dump=0)>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.875\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mEntering 'OptimizerFactory' (args=({'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, <generator object Module.parameters at 0x7d391c00f220>), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.875\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mExiting 'OptimizerFactory' (result=OptimizerFactory(config={'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, parameters_policy=<generator object Module.parameters at 0x7d391c00f220>))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating AdamW instance with args: {'lr': 0.001, 'weight_decay': 0.01}\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.879\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m40\u001b[0m - \u001b[34m\u001b[1mEntering 'SchedulerFactory' (args=({'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, [AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")]), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.879\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m40\u001b[0m - \u001b[34m\u001b[1mExiting 'SchedulerFactory' (result=SchedulerFactory(config={'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, optimizers=[AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")]))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating ReduceLROnPlateau instance with args: {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating WarmupConstantSchedule instance with args: {'warmup_steps': 100}\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.881\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'WarmupConstantSchedule' (args=(AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "),), kwargs={'warmup_steps': 100})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.881\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'WarmupConstantSchedule' (result=<WarmupConstantSchedule(optimizer=AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.001\n",
            "    lr: 0.0\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "), warmup_steps=100, last_epoch=0, lr_lambda = <function WarmupConstantSchedule.__post_init__.<locals>.lr_lambda at 0x7d391c496fc0>)>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:48.881\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mEntering 'Engine' (args=(Namespace(model='SepReformer_Base_WSJ0', engine_mode='train', sample_file=None, out_wav_dir=None), {'dataset': {'max_len': 32000, 'sampling_rate': 8000, 'scp_dir': 'data/custom', 'train': {'mixture': 'train_mix_clean.scp', 'spk1': 'train_s1.scp', 'spk2': 'train_s2.scp', 'dynamic_mixing': True}, 'valid': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}, 'test': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}}, 'dataloader': {'batch_size': 1, 'pin_memory': True, 'num_workers': 4, 'drop_last': True}, 'model': {'num_stages': 4, 'num_spks': 2, 'module_audio_enc': {'in_channels': 1, 'out_channels': 256, 'kernel_size': 16, 'stride': 4, 'groups': 1, 'bias': False}, 'module_feature_projector': {'num_channels': 256, 'in_channels': 256, 'out_channels': 128, 'kernel_size': 1, 'bias': False}, 'module_separator': {'num_stages': 4, 'relative_positional_encoding': {'in_channels': 128, 'num_heads': 8, 'maxlen': 2000, 'embed_v': False}, 'enc_stage': {'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'down_conv_layer': {'in_channels': 128, 'samp_kernel_size': 5}}, 'spk_split_stage': {'in_channels': 128, 'num_spks': 2}, 'simple_fusion': {'out_channels': 128}, 'dec_stage': {'num_spks': 2, 'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'spk_attention': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}}}, 'module_output_layer': {'in_channels': 256, 'out_channels': 128, 'num_spks': 2}, 'module_audio_dec': {'in_channels': 256, 'out_channels': 1, 'kernel_size': 16, 'stride': 4, 'bias': False}}, 'criterion': {'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, 'optimizer': {'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, 'scheduler': {'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, 'check_computations': {'dummy_len': 16000}, 'engine': {'max_epoch': 90, 'gpuid': '0', 'mvn': False, 'clip_norm': 5, 'start_scheduling': 10, 'test_epochs': [50, 90]}}, Model(\n",
            "  (audio_encoder): AudioEncoder(\n",
            "    (conv1d): Conv1d(1, 256, kernel_size=(16,), stride=(4,), bias=False)\n",
            "    (gelu): GELU(approximate='none')\n",
            "  )\n",
            "  (feature_projector): FeatureProjector(\n",
            "    (norm): GroupNorm(1, 256, eps=1e-08, affine=True)\n",
            "    (conv1d): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "  )\n",
            "  (separator): Separator(\n",
            "    (pos_emb): RelativePositionalEncoding(\n",
            "      (pe_k): Embedding(4000, 16)\n",
            "    )\n",
            "    (enc_stages): ModuleList(\n",
            "      (0-3): 4 x SepEncStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downconv): DownConvLayer(\n",
            "          (down_conv): Conv1d(128, 128, kernel_size=(5,), stride=(2,), padding=(2,), groups=128)\n",
            "          (BN): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (gelu): GELU(approximate='none')\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (bottleneck_G): SepEncStage(\n",
            "      (g_block_1): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_1): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (g_block_2): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_2): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (spk_split_block): AttentionSpeakerSplit(\n",
            "      (cross_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff_gate): Sequential(\n",
            "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (1): Linear(in_features=128, out_features=256, bias=True)\n",
            "        (2): GLU(dim=-1)\n",
            "      )\n",
            "      (ff_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (res_proj): Linear(in_features=128, out_features=128, bias=False)\n",
            "      (out_norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "    )\n",
            "    (simple_fusion): ModuleList(\n",
            "      (0-3): 4 x Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "    (dec_stages): ModuleList(\n",
            "      (0-3): 4 x SepDecStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_1): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_2): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_3): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_3): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_3): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (out_layer): OutputLayer(\n",
            "    (spe_block): Masking(\n",
            "      (gate_act): ReLU()\n",
            "    )\n",
            "    (end_conv1x1): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (1): GLU(dim=-1)\n",
            "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (audio_decoder): AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  (out_layer_bn): ModuleList(\n",
            "    (0-3): 4 x OutputLayer(\n",
            "      (spe_block): Masking(\n",
            "        (gate_act): ReLU()\n",
            "      )\n",
            "      (end_conv1x1): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): GLU(dim=-1)\n",
            "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder_bn): ModuleList(\n",
            "    (0-3): 4 x AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  )\n",
            "), {'train': <torch.utils.data.dataloader.DataLoader object at 0x7d391ca49a30>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7d391c5fdd30>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7d391ca0f260>}, [<PIT_SISNR_mag(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', num_stages=4, num_spks=2, scale_inv=True, mel_opt=False, stft = [STFT instance for 4 layers], mel_fb=Identity)>, <PIT_SISNR_time(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>, <PIT_SISNRi(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>, <PIT_SDRi(device=device(type='cuda', index=0), dump=0)>], [AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.001\n",
            "    lr: 0.0\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")], [<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7d391bcc62d0>, <WarmupConstantSchedule(optimizer=AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.001\n",
            "    lr: 0.0\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "), warmup_steps=100, last_epoch=0, lr_lambda = <function WarmupConstantSchedule.__post_init__.<locals>.lr_lambda at 0x7d391c496fc0>)>], (0,), device(type='cuda', index=0)), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:50.080\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_engine\u001b[0m:\u001b[36mmodel_params_mac_summary\u001b[0m:\u001b[36m138\u001b[0m - \u001b[1mptflops: MACs: 43.99 GMac, Params: 14.54\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:50.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_engine\u001b[0m:\u001b[36mmodel_params_mac_summary\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1mthop: MACs: 42.577498368 GMac, Params: 14.378752\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:50.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_engine\u001b[0m:\u001b[36mmodel_params_mac_summary\u001b[0m:\u001b[36m150\u001b[0m - \u001b[1mtorchinfo: MACs: 3194.231296 GMac, Params: 14.54464\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:50.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mClip gradient by 2-norm 5\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:50.683\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mExiting 'Engine' (result=<models.SepReformer_Base_WSJ0.engine.Engine object at 0x7d391c1e7800>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:50.683\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m47\u001b[0m - \u001b[34m\u001b[1mEntering 'run' (args=(<models.SepReformer_Base_WSJ0.engine.Engine object at 0x7d391c1e7800>,), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:50.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1m[INIT] Loss(time/mini-batch) \n",
            " - Epoch  1: Loss_t = 0.0000 dB | Loss_f = 0.0000 dB | Speed = (0.00s)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:22:50.685\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m196\u001b[0m - \u001b[34m\u001b[1mEntering '_train' (args=(<models.SepReformer_Base_WSJ0.engine.Engine object at 0x7d391c1e7800>, <torch.utils.data.dataloader.DataLoader object at 0x7d391ca49a30>, 1), kwargs={})\u001b[0m\n",
            "100% 160/160 [01:34<00:00,  1.70batches/s, T_Loss=0.951, F_Loss_0=1.53, F_Loss_1=0.87, F_Loss_2=2.05, F_Loss_3=3.55]\n",
            "\u001b[32m2026-02-18 12:24:24.966\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m196\u001b[0m - \u001b[34m\u001b[1mExiting '_train' (result=(0.9506262234412134, 2.0015052875853145, 160))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:24:24.967\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mEntering '_validate' (args=(<models.SepReformer_Base_WSJ0.engine.Engine object at 0x7d391c1e7800>, <torch.utils.data.dataloader.DataLoader object at 0x7d391c5fdd30>), kwargs={})\u001b[0m\n",
            "100% 40/40 [00:20<00:00,  1.92batches/s, T_Loss=-0.484, F_Loss_0=-0.54, F_Loss_1=-0.332, F_Loss_2=-0.427, F_Loss_3=-0.757]\n",
            "\u001b[32m2026-02-18 12:24:45.810\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mExiting '_validate' (result=(-0.4838846422731876, -0.5140242492780089, 40))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:24:45.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1m[TRAIN] Loss(time/mini-batch) \n",
            " - Epoch  1: Loss_t = 0.9506 dB | Loss_f = 2.0015 dB | Speed = (94.28s/160)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:24:45.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1m[VALID] Loss(time/mini-batch) \n",
            " - Epoch  1: Loss_t = -0.4839 dB | Loss_f = -0.5140 dB | Speed = (20.84s/40)\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/SepReformer/run.py\", line 32, in <module>\n",
            "    main_module.main(args)\n",
            "  File \"/content/SepReformer/utils/decorators.py\", line 12, in wrapped\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/main.py\", line 47, in main\n",
            "    engine.run()\n",
            "  File \"/content/SepReformer/utils/decorators.py\", line 12, in wrapped\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/engine.py\", line 209, in run\n",
            "    valid_loss_best = util_engine.save_checkpoint_per_best(valid_loss_best, valid_loss_src_time, train_loss_src_time, epoch, self.model, self.main_optimizer, self.checkpoint_path, self.wandb_run)\n",
            "                                                                                                                                                                                    ^^^^^^^^^^^^^^\n",
            "AttributeError: 'Engine' object has no attribute 'wandb_run'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the engine file\n",
        "engine_path = 'models/SepReformer_Base_WSJ0/engine.py'\n",
        "print(f\"ğŸ”§ Patching {engine_path} to fix AttributeError and typos...\")\n",
        "\n",
        "if os.path.exists(engine_path):\n",
        "    with open(engine_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    new_lines = []\n",
        "    for line in lines:\n",
        "        # Fix 1: Remove 'self.wandb_run' causing AttributeError\n",
        "        if 'util_engine.save_checkpoint_per_best' in line and 'self.wandb_run' in line:\n",
        "            # Remove the argument. Assuming format: ..., self.checkpoint_path, self.wandb_run)\n",
        "            line = line.replace(', self.wandb_run', '')\n",
        "            print(\"  âœ… Removed undefined 'self.wandb_run' argument.\")\n",
        "\n",
        "        # Fix 2: 'add_scalars' should be 'add_scalar' for single value (Learning Rate)\n",
        "        # This prevents a future error during logging\n",
        "        if 'writer_src.add_scalars(\"Learning Rate\"' in line:\n",
        "            line = line.replace('add_scalars', 'add_scalar')\n",
        "            print(\"  âœ… Fixed 'add_scalars' -> 'add_scalar' for Learning Rate.\")\n",
        "\n",
        "        new_lines.append(line)\n",
        "\n",
        "    with open(engine_path, 'w') as f:\n",
        "        f.writelines(new_lines)\n",
        "    print(\"âœ… engine.py patched.\")\n",
        "else:\n",
        "    print(f\"âŒ Error: {engine_path} not found.\")\n",
        "\n",
        "print(\"ğŸš€ Restarting Training...\")\n",
        "!python run.py --model SepReformer_Base_WSJ0 --engine-mode train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZId9VKbtBeLP",
        "outputId": "30a83136-7a9a-4469-bafa-6779d92be193"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Patching models/SepReformer_Base_WSJ0/engine.py to fix AttributeError and typos...\n",
            "  âœ… Removed undefined 'self.wandb_run' argument.\n",
            "  âœ… Fixed 'add_scalars' -> 'add_scalar' for Learning Rate.\n",
            "âœ… engine.py patched.\n",
            "ğŸš€ Restarting Training...\n",
            "2026-02-18 12:24:56.174332: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-02-18 12:24:56.192745: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771417496.215039    3935 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771417496.222430    3935 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771417496.241131    3935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771417496.241167    3935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771417496.241170    3935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771417496.241173    3935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-18 12:24:56.246012: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m2026-02-18 12:25:02.039\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mEntering 'main' (args=(Namespace(model='SepReformer_Base_WSJ0', engine_mode='train', sample_file=None, out_wav_dir=None),), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.040\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m20\u001b[0m - \u001b[34m\u001b[1mEntering 'parse_yaml' (args=('/content/SepReformer/models/SepReformer_Base_WSJ0/configs.yaml',), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.049\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m20\u001b[0m - \u001b[34m\u001b[1mExiting 'parse_yaml' (result={'project': '[Project] SepReformer', 'notes': 'Training on custom dataset - Fixed Scheduler', 'config': {'dataset': {'max_len': 32000, 'sampling_rate': 8000, 'scp_dir': 'data/custom', 'train': {'mixture': 'train_mix_clean.scp', 'spk1': 'train_s1.scp', 'spk2': 'train_s2.scp', 'dynamic_mixing': True}, 'valid': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}, 'test': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}}, 'dataloader': {'batch_size': 1, 'pin_memory': True, 'num_workers': 4, 'drop_last': True}, 'model': {'num_stages': 4, 'num_spks': 2, 'module_audio_enc': {'in_channels': 1, 'out_channels': 256, 'kernel_size': 16, 'stride': 4, 'groups': 1, 'bias': False}, 'module_feature_projector': {'num_channels': 256, 'in_channels': 256, 'out_channels': 128, 'kernel_size': 1, 'bias': False}, 'module_separator': {'num_stages': 4, 'relative_positional_encoding': {'in_channels': 128, 'num_heads': 8, 'maxlen': 2000, 'embed_v': False}, 'enc_stage': {'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'down_conv_layer': {'in_channels': 128, 'samp_kernel_size': 5}}, 'spk_split_stage': {'in_channels': 128, 'num_spks': 2}, 'simple_fusion': {'out_channels': 128}, 'dec_stage': {'num_spks': 2, 'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'spk_attention': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}}}, 'module_output_layer': {'in_channels': 256, 'out_channels': 128, 'num_spks': 2}, 'module_audio_dec': {'in_channels': 256, 'out_channels': 1, 'kernel_size': 16, 'stride': 4, 'bias': False}}, 'criterion': {'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, 'optimizer': {'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, 'scheduler': {'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, 'check_computations': {'dummy_len': 16000}, 'engine': {'max_epoch': 90, 'gpuid': '0', 'mvn': False, 'clip_norm': 5, 'start_scheduling': 10, 'test_epochs': [50, 90]}}})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.049\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m26\u001b[0m - \u001b[34m\u001b[1mEntering 'get_dataloaders' (args=(Namespace(model='SepReformer_Base_WSJ0', engine_mode='train', sample_file=None, out_wav_dir=None), {'max_len': 32000, 'sampling_rate': 8000, 'scp_dir': 'data/custom', 'train': {'mixture': 'train_mix_clean.scp', 'spk1': 'train_s1.scp', 'spk2': 'train_s2.scp', 'dynamic_mixing': True}, 'valid': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}, 'test': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}}, {'batch_size': 1, 'pin_memory': True, 'num_workers': 4, 'drop_last': True}), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.050\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mEntering 'MyDataset' (args=(), kwargs={'max_len': 32000, 'fs': 8000, 'partition': 'train', 'wave_scp_srcs': ['data/custom/train_s1.scp', 'data/custom/train_s2.scp'], 'wave_scp_mix': 'data/custom/train_mix_clean.scp', 'dynamic_mixing': True})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mCreate MyDataset for data/custom/train_mix_clean.scp with 160 utterances\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mExiting 'MyDataset' (result=<models.SepReformer_Base_WSJ0.dataset.MyDataset object at 0x79b8e42ec080>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mEntering 'MyDataset' (args=(), kwargs={'max_len': 32000, 'fs': 8000, 'partition': 'valid', 'wave_scp_srcs': ['data/custom/val_s1.scp', 'data/custom/val_s2.scp'], 'wave_scp_mix': 'data/custom/val_mix_clean.scp', 'dynamic_mixing': False})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mCreate MyDataset for data/custom/val_mix_clean.scp with 40 utterances\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mExiting 'MyDataset' (result=<models.SepReformer_Base_WSJ0.dataset.MyDataset object at 0x79b8e42a78f0>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mEntering 'MyDataset' (args=(), kwargs={'max_len': 32000, 'fs': 8000, 'partition': 'test', 'wave_scp_srcs': ['data/custom/val_s1.scp', 'data/custom/val_s2.scp'], 'wave_scp_mix': 'data/custom/val_mix_clean.scp', 'dynamic_mixing': False})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mCreate MyDataset for data/custom/val_mix_clean.scp with 40 utterances\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.052\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mExiting 'MyDataset' (result=<models.SepReformer_Base_WSJ0.dataset.MyDataset object at 0x79b8e42ec980>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.052\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m26\u001b[0m - \u001b[34m\u001b[1mExiting 'get_dataloaders' (result={'train': <torch.utils.data.dataloader.DataLoader object at 0x79bb26518650>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x79b9e6119550>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x79b8e42ec1a0>})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.052\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mEntering 'Model' (args=(), kwargs={'num_stages': 4, 'num_spks': 2, 'module_audio_enc': {'in_channels': 1, 'out_channels': 256, 'kernel_size': 16, 'stride': 4, 'groups': 1, 'bias': False}, 'module_feature_projector': {'num_channels': 256, 'in_channels': 256, 'out_channels': 128, 'kernel_size': 1, 'bias': False}, 'module_separator': {'num_stages': 4, 'relative_positional_encoding': {'in_channels': 128, 'num_heads': 8, 'maxlen': 2000, 'embed_v': False}, 'enc_stage': {'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'down_conv_layer': {'in_channels': 128, 'samp_kernel_size': 5}}, 'spk_split_stage': {'in_channels': 128, 'num_spks': 2}, 'simple_fusion': {'out_channels': 128}, 'dec_stage': {'num_spks': 2, 'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'spk_attention': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}}}, 'module_output_layer': {'in_channels': 256, 'out_channels': 128, 'num_spks': 2}, 'module_audio_dec': {'in_channels': 256, 'out_channels': 1, 'kernel_size': 16, 'stride': 4, 'bias': False}})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.230\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mExiting 'Model' (result=Model(\n",
            "  (audio_encoder): AudioEncoder(\n",
            "    (conv1d): Conv1d(1, 256, kernel_size=(16,), stride=(4,), bias=False)\n",
            "    (gelu): GELU(approximate='none')\n",
            "  )\n",
            "  (feature_projector): FeatureProjector(\n",
            "    (norm): GroupNorm(1, 256, eps=1e-08, affine=True)\n",
            "    (conv1d): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "  )\n",
            "  (separator): Separator(\n",
            "    (pos_emb): RelativePositionalEncoding(\n",
            "      (pe_k): Embedding(4000, 16)\n",
            "    )\n",
            "    (enc_stages): ModuleList(\n",
            "      (0-3): 4 x SepEncStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downconv): DownConvLayer(\n",
            "          (down_conv): Conv1d(128, 128, kernel_size=(5,), stride=(2,), padding=(2,), groups=128)\n",
            "          (BN): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (gelu): GELU(approximate='none')\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (bottleneck_G): SepEncStage(\n",
            "      (g_block_1): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_1): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (g_block_2): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_2): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (spk_split_block): AttentionSpeakerSplit(\n",
            "      (cross_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff_gate): Sequential(\n",
            "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (1): Linear(in_features=128, out_features=256, bias=True)\n",
            "        (2): GLU(dim=-1)\n",
            "      )\n",
            "      (ff_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (res_proj): Linear(in_features=128, out_features=128, bias=False)\n",
            "      (out_norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "    )\n",
            "    (simple_fusion): ModuleList(\n",
            "      (0-3): 4 x Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "    (dec_stages): ModuleList(\n",
            "      (0-3): 4 x SepDecStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_1): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_2): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_3): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_3): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_3): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (out_layer): OutputLayer(\n",
            "    (spe_block): Masking(\n",
            "      (gate_act): ReLU()\n",
            "    )\n",
            "    (end_conv1x1): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (1): GLU(dim=-1)\n",
            "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (audio_decoder): AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  (out_layer_bn): ModuleList(\n",
            "    (0-3): 4 x OutputLayer(\n",
            "      (spe_block): Masking(\n",
            "        (gate_act): ReLU()\n",
            "      )\n",
            "      (end_conv1x1): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): GLU(dim=-1)\n",
            "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder_bn): ModuleList(\n",
            "    (0-3): 4 x AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  )\n",
            "))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.308\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mEntering 'CriterionFactory' (args=({'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, device(type='cuda', index=0)), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.308\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mExiting 'CriterionFactory' (result=CriterionFactory(config={'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, device=device(type='cuda', index=0)))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SISNR_mag instance with args: {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.387\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SISNR_mag' (args=(device(type='cuda', index=0),), kwargs={'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.387\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.550\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.685\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.687\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.690\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.691\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.694\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.695\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.698\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SISNR_mag' (result=<PIT_SISNR_mag(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', num_stages=4, num_spks=2, scale_inv=True, mel_opt=False, stft = [STFT instance for 4 layers], mel_fb=Identity)>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SISNR_time instance with args: {'num_spks': 2, 'scale_inv': True}\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.698\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SISNR_time' (args=(device(type='cuda', index=0),), kwargs={'num_spks': 2, 'scale_inv': True})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.698\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SISNR_time' (result=<PIT_SISNR_time(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SISNRi instance with args: {'num_spks': 2, 'scale_inv': True}\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.698\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SISNRi' (args=(device(type='cuda', index=0),), kwargs={'num_spks': 2, 'scale_inv': True})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.698\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SISNRi' (result=<PIT_SISNRi(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SDRi instance with args: {'dump': 0}\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.698\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SDRi' (args=(device(type='cuda', index=0),), kwargs={'dump': 0})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.698\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SDRi' (result=<PIT_SDRi(device=device(type='cuda', index=0), dump=0)>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.699\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mEntering 'OptimizerFactory' (args=({'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, <generator object Module.parameters at 0x79b8e3e6cc80>), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.699\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mExiting 'OptimizerFactory' (result=OptimizerFactory(config={'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, parameters_policy=<generator object Module.parameters at 0x79b8e3e6cc80>))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating AdamW instance with args: {'lr': 0.001, 'weight_decay': 0.01}\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.703\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m40\u001b[0m - \u001b[34m\u001b[1mEntering 'SchedulerFactory' (args=({'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, [AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")]), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.703\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m40\u001b[0m - \u001b[34m\u001b[1mExiting 'SchedulerFactory' (result=SchedulerFactory(config={'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, optimizers=[AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")]))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating ReduceLROnPlateau instance with args: {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating WarmupConstantSchedule instance with args: {'warmup_steps': 100}\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.704\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'WarmupConstantSchedule' (args=(AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "),), kwargs={'warmup_steps': 100})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.705\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'WarmupConstantSchedule' (result=<WarmupConstantSchedule(optimizer=AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.001\n",
            "    lr: 0.0\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "), warmup_steps=100, last_epoch=0, lr_lambda = <function WarmupConstantSchedule.__post_init__.<locals>.lr_lambda at 0x79b8e4123600>)>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:02.705\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mEntering 'Engine' (args=(Namespace(model='SepReformer_Base_WSJ0', engine_mode='train', sample_file=None, out_wav_dir=None), {'dataset': {'max_len': 32000, 'sampling_rate': 8000, 'scp_dir': 'data/custom', 'train': {'mixture': 'train_mix_clean.scp', 'spk1': 'train_s1.scp', 'spk2': 'train_s2.scp', 'dynamic_mixing': True}, 'valid': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}, 'test': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}}, 'dataloader': {'batch_size': 1, 'pin_memory': True, 'num_workers': 4, 'drop_last': True}, 'model': {'num_stages': 4, 'num_spks': 2, 'module_audio_enc': {'in_channels': 1, 'out_channels': 256, 'kernel_size': 16, 'stride': 4, 'groups': 1, 'bias': False}, 'module_feature_projector': {'num_channels': 256, 'in_channels': 256, 'out_channels': 128, 'kernel_size': 1, 'bias': False}, 'module_separator': {'num_stages': 4, 'relative_positional_encoding': {'in_channels': 128, 'num_heads': 8, 'maxlen': 2000, 'embed_v': False}, 'enc_stage': {'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'down_conv_layer': {'in_channels': 128, 'samp_kernel_size': 5}}, 'spk_split_stage': {'in_channels': 128, 'num_spks': 2}, 'simple_fusion': {'out_channels': 128}, 'dec_stage': {'num_spks': 2, 'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'spk_attention': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}}}, 'module_output_layer': {'in_channels': 256, 'out_channels': 128, 'num_spks': 2}, 'module_audio_dec': {'in_channels': 256, 'out_channels': 1, 'kernel_size': 16, 'stride': 4, 'bias': False}}, 'criterion': {'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, 'optimizer': {'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, 'scheduler': {'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, 'check_computations': {'dummy_len': 16000}, 'engine': {'max_epoch': 90, 'gpuid': '0', 'mvn': False, 'clip_norm': 5, 'start_scheduling': 10, 'test_epochs': [50, 90]}}, Model(\n",
            "  (audio_encoder): AudioEncoder(\n",
            "    (conv1d): Conv1d(1, 256, kernel_size=(16,), stride=(4,), bias=False)\n",
            "    (gelu): GELU(approximate='none')\n",
            "  )\n",
            "  (feature_projector): FeatureProjector(\n",
            "    (norm): GroupNorm(1, 256, eps=1e-08, affine=True)\n",
            "    (conv1d): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "  )\n",
            "  (separator): Separator(\n",
            "    (pos_emb): RelativePositionalEncoding(\n",
            "      (pe_k): Embedding(4000, 16)\n",
            "    )\n",
            "    (enc_stages): ModuleList(\n",
            "      (0-3): 4 x SepEncStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downconv): DownConvLayer(\n",
            "          (down_conv): Conv1d(128, 128, kernel_size=(5,), stride=(2,), padding=(2,), groups=128)\n",
            "          (BN): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (gelu): GELU(approximate='none')\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (bottleneck_G): SepEncStage(\n",
            "      (g_block_1): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_1): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (g_block_2): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_2): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (spk_split_block): AttentionSpeakerSplit(\n",
            "      (cross_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (ff_gate): Sequential(\n",
            "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (1): Linear(in_features=128, out_features=256, bias=True)\n",
            "        (2): GLU(dim=-1)\n",
            "      )\n",
            "      (ff_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (res_proj): Linear(in_features=128, out_features=128, bias=False)\n",
            "      (out_norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "    )\n",
            "    (simple_fusion): ModuleList(\n",
            "      (0-3): 4 x Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "    (dec_stages): ModuleList(\n",
            "      (0-3): 4 x SepDecStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_1): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_2): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_3): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_3): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_3): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (out_layer): OutputLayer(\n",
            "    (spe_block): Masking(\n",
            "      (gate_act): ReLU()\n",
            "    )\n",
            "    (end_conv1x1): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (1): GLU(dim=-1)\n",
            "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (audio_decoder): AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  (out_layer_bn): ModuleList(\n",
            "    (0-3): 4 x OutputLayer(\n",
            "      (spe_block): Masking(\n",
            "        (gate_act): ReLU()\n",
            "      )\n",
            "      (end_conv1x1): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): GLU(dim=-1)\n",
            "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder_bn): ModuleList(\n",
            "    (0-3): 4 x AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  )\n",
            "), {'train': <torch.utils.data.dataloader.DataLoader object at 0x79bb26518650>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x79b9e6119550>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x79b8e42ec1a0>}, [<PIT_SISNR_mag(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', num_stages=4, num_spks=2, scale_inv=True, mel_opt=False, stft = [STFT instance for 4 layers], mel_fb=Identity)>, <PIT_SISNR_time(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>, <PIT_SISNRi(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>, <PIT_SDRi(device=device(type='cuda', index=0), dump=0)>], [AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.001\n",
            "    lr: 0.0\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")], [<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x79b8e4408980>, <WarmupConstantSchedule(optimizer=AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.001\n",
            "    lr: 0.0\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "), warmup_steps=100, last_epoch=0, lr_lambda = <function WarmupConstantSchedule.__post_init__.<locals>.lr_lambda at 0x79b8e4123600>)>], (0,), device(type='cuda', index=0)), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:03.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_engine\u001b[0m:\u001b[36mmodel_params_mac_summary\u001b[0m:\u001b[36m138\u001b[0m - \u001b[1mptflops: MACs: 43.99 GMac, Params: 14.54\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:03.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_engine\u001b[0m:\u001b[36mmodel_params_mac_summary\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1mthop: MACs: 42.577498368 GMac, Params: 14.378752\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:04.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_engine\u001b[0m:\u001b[36mmodel_params_mac_summary\u001b[0m:\u001b[36m150\u001b[0m - \u001b[1mtorchinfo: MACs: 3194.231296 GMac, Params: 14.54464\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:04.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mClip gradient by 2-norm 5\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:04.035\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mExiting 'Engine' (result=<models.SepReformer_Base_WSJ0.engine.Engine object at 0x79b8e3a1a540>)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:04.035\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m47\u001b[0m - \u001b[34m\u001b[1mEntering 'run' (args=(<models.SepReformer_Base_WSJ0.engine.Engine object at 0x79b8e3a1a540>,), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:04.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1m[INIT] Loss(time/mini-batch) \n",
            " - Epoch  1: Loss_t = 0.0000 dB | Loss_f = 0.0000 dB | Speed = (0.00s)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:25:04.037\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m196\u001b[0m - \u001b[34m\u001b[1mEntering '_train' (args=(<models.SepReformer_Base_WSJ0.engine.Engine object at 0x79b8e3a1a540>, <torch.utils.data.dataloader.DataLoader object at 0x79bb26518650>, 1), kwargs={})\u001b[0m\n",
            "100% 160/160 [01:20<00:00,  1.98batches/s, T_Loss=1.5, F_Loss_0=1.68, F_Loss_1=1.6, F_Loss_2=1.12, F_Loss_3=3.28]\n",
            "\u001b[32m2026-02-18 12:26:24.722\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m196\u001b[0m - \u001b[34m\u001b[1mExiting '_train' (result=(1.49868856491521, 1.920925613993313, 160))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:26:24.722\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mEntering '_validate' (args=(<models.SepReformer_Base_WSJ0.engine.Engine object at 0x79b8e3a1a540>, <torch.utils.data.dataloader.DataLoader object at 0x79b9e6119550>), kwargs={})\u001b[0m\n",
            "100% 40/40 [00:06<00:00,  5.95batches/s, T_Loss=-0.167, F_Loss_0=-0.492, F_Loss_1=0.23, F_Loss_2=0.0799, F_Loss_3=2.45]\n",
            "\u001b[32m2026-02-18 12:26:31.448\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mExiting '_validate' (result=(-0.16693585962057114, 0.5661944024148398, 40))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:26:31.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1m[TRAIN] Loss(time/mini-batch) \n",
            " - Epoch  1: Loss_t = 1.4987 dB | Loss_f = 1.9209 dB | Speed = (80.69s/160)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:26:31.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m203\u001b[0m - \u001b[1m[VALID] Loss(time/mini-batch) \n",
            " - Epoch  1: Loss_t = -0.1669 dB | Loss_f = 0.5662 dB | Speed = (6.73s/40)\u001b[0m\n",
            "\u001b[32m2026-02-18 12:26:32.071\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m196\u001b[0m - \u001b[34m\u001b[1mEntering '_train' (args=(<models.SepReformer_Base_WSJ0.engine.Engine object at 0x79b8e3a1a540>, <torch.utils.data.dataloader.DataLoader object at 0x79bb26518650>, 2), kwargs={})\u001b[0m\n",
            "100% 160/160 [01:17<00:00,  2.06batches/s, T_Loss=-0.815, F_Loss_0=-1.17, F_Loss_1=-1.32, F_Loss_2=-1.32, F_Loss_3=-1.95]\n",
            "\u001b[32m2026-02-18 12:27:49.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m196\u001b[0m - \u001b[34m\u001b[1mExiting '_train' (result=(-0.8148168873973191, -1.4419410192407667, 160))\u001b[0m\n",
            "\u001b[32m2026-02-18 12:27:49.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m199\u001b[0m - \u001b[34m\u001b[1mEntering '_validate' (args=(<models.SepReformer_Base_WSJ0.engine.Engine object at 0x79b8e3a1a540>, <torch.utils.data.dataloader.DataLoader object at 0x79b9e6119550>), kwargs={})\u001b[0m\n",
            "  8% 3/40 [00:01<00:15,  2.35batches/s, T_Loss=-0.905, F_Loss_0=-0.947, F_Loss_1=-1.06, F_Loss_2=-1.09, F_Loss_3=-2.12]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79b9f8c48ea0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1599, in _shutdown_workers\n",
            "    self._worker_result_queue.put((None, None))\n",
            "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 94, in put\n",
            "    self._start_thread()\n",
            "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 180, in _start_thread\n",
            "    self._thread = threading.Thread(\n",
            "                   ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 911, in __init__\n",
            "    name = str(name)\n",
            "           ^^^^^^^^^\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/SepReformer/run.py\", line 32, in <module>\n",
            "    main_module.main(args)\n",
            "  File \"/content/SepReformer/utils/decorators.py\", line 12, in wrapped\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/main.py\", line 47, in main\n",
            "    engine.run()\n",
            "  File \"/content/SepReformer/utils/decorators.py\", line 12, in wrapped\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/engine.py\", line 199, in run\n",
            "    valid_loss_src_time, valid_loss_src_freq, valid_num_batch = self._validate(self.dataloaders['valid'])\n",
            "                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/utils/decorators.py\", line 12, in wrapped\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/engine.py\", line 98, in _validate\n",
            "    estim_src, estim_src_bn = torch.nn.parallel.data_parallel(self.model, nnet_input, device_ids=self.gpuid)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/data_parallel.py\", line 282, in data_parallel\n",
            "    return module(*inputs[0], **module_kwargs[0])\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/model.py\", line 41, in forward\n",
            "    last_stage_output, each_stage_outputs = self.separator(projected_feature)\n",
            "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/modules/module.py\", line 215, in forward\n",
            "    x, _ = self.dec_stages[idx](x, pos_k)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/modules/module.py\", line 150, in forward\n",
            "    x = self.g_block_1(x, pos_k)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/modules/network.py\", line 205, in forward\n",
            "    x = self.block['ega'](x, pos_k)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/modules/network.py\", line 153, in forward\n",
            "    x = x + self.block['linear'](x) * x_downup\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py\", line 229, in forward\n",
            "    return F.layer_norm(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\", line 2901, in layer_norm\n",
            "    return torch.layer_norm(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "  8% 3/40 [00:02<00:26,  1.38batches/s, T_Loss=-0.905, F_Loss_0=-0.947, F_Loss_1=-1.06, F_Loss_2=-1.09, F_Loss_3=-2.12]\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l models/SepReformer_Base_WSJ0/log/scratch_weights/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFCTfJ574Eiu",
        "outputId": "c6bc1143-d7c6-42fe-f1e7-62863b71305a",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 172560\n",
            "-rw-r--r-- 1 root root 176696025 Feb 18 12:26 epoch.0001.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "# ×‘×—×™×¨×ª ×§×•×‘×¥ ××§×¨××™\n",
        "mix_dir = '/content/drive/MyDrive/×ª×•××¨/speech/mix_clean'\n",
        "files = [f for f in os.listdir(mix_dir) if f.endswith('.wav')]\n",
        "test_file = random.choice(files)\n",
        "test_file_path = os.path.join(mix_dir, test_file)\n",
        "\n",
        "print(f\"Testing on: {test_file_path}\")\n",
        "\n",
        "# ×”×¤×§×•×“×” ×”××ª×•×§× ×ª (×©×™× ×œ×‘ ×œ××§×¤×™× ×‘-out-wav-dir)\n",
        "!python run.py \\\n",
        "  --model SepReformer_Base_WSJ0 \\\n",
        "  --engine-mode infer_sample \\\n",
        "  --sample-file \"{test_file_path}\" \\\n",
        "  --out-wav-dir \"results_debug\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st6anh3k695s",
        "outputId": "a12e664d-cac8-4b99-aa35-5c2f884713c0",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on: /content/drive/MyDrive/×ª×•××¨/speech/mix_clean/mix_00046.wav\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/rpc/server_process_global_profiler.py\", line 7, in <module>\n",
            "    from torch.autograd.profiler_legacy import profile\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\", line 32, in <module>\n",
            "    from .gradcheck import gradcheck, gradgradcheck\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/gradcheck.py\", line 11, in <module>\n",
            "    import torch.testing\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/testing/__init__.py\", line 4, in <module>\n",
            "    from ._comparison import assert_allclose, assert_close as assert_close\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/testing/_comparison.py\", line 422, in <module>\n",
            "    class ObjectPair(Pair):\n",
            "  File \"<frozen abc>\", line 106, in __new__\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/SepReformer/run.py\", line 31, in <module>\n",
            "    main_module = importlib.import_module(f\"models.{args.model}.main\")\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/main.py\", line 2, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 2161, in <module>\n",
            "    from torch import _VF as _VF, functional as functional  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/functional.py\", line 8, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/__init__.py\", line 8, in <module>\n",
            "    from torch.nn.modules import *  # usort: skip # noqa: F403\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/__init__.py\", line 2, in <module>\n",
            "    from .linear import Bilinear, Identity, LazyLinear, Linear  # usort: skip\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\", line 7, in <module>\n",
            "    from torch.nn import functional as F, init\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\", line 11, in <module>\n",
            "    from torch._jit_internal import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_jit_internal.py\", line 44, in <module>\n",
            "    import torch.distributed.rpc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/rpc/__init__.py\", line 85, in <module>\n",
            "    from .server_process_global_profiler import _server_process_global_profile\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1334, in _find_and_load_unlocked\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define Paths\n",
        "checkpoint_path = \"/content/SepReformer/models/SepReformer_Base_WSJ0/log/scratch_weights/\"\n",
        "# Find the latest checkpoint automatically\n",
        "checkpoints = sorted(glob.glob(os.path.join(checkpoint_path, \"epoch.*.pth\")))\n",
        "latest_ckpt = checkpoints[-1]\n",
        "print(f\"â­ Using Checkpoint: {os.path.basename(latest_ckpt)}\")\n",
        "\n",
        "# 2. Pick a random test file\n",
        "mix_dir = '/content/drive/MyDrive/×ª×•××¨/speech/mix_clean'\n",
        "files = [f for f in os.listdir(mix_dir) if f.endswith('.wav') and '_in' not in f and '_out' not in f]\n",
        "test_file = random.choice(files)\n",
        "test_file_path = os.path.join(mix_dir, test_file)\n",
        "\n",
        "print(f\"ğŸµ Separating: {test_file}\")\n",
        "\n",
        "# 3. Run Inference\n",
        "!python run.py \\\n",
        "  --model SepReformer_Base_WSJ0 \\\n",
        "  --engine-mode infer_sample \\\n",
        "  --sample-file \"{test_file_path}\" \\\n",
        "  --out-wav-dir \"final_results_90ep\" > /dev/null 2>&1\n",
        "\n",
        "# 4. Locate and Move Files (Handling the path bug we saw earlier)\n",
        "# The model likely saved them in the source folder again due to the bug\n",
        "base_name = os.path.splitext(test_file)[0]\n",
        "path_mix = os.path.join(mix_dir, f\"{base_name}_in.wav\")\n",
        "path_s1 = os.path.join(mix_dir, f\"{base_name}_out_0.wav\")\n",
        "path_s2 = os.path.join(mix_dir, f\"{base_name}_out_1.wav\")\n",
        "\n",
        "if os.path.exists(path_s1):\n",
        "    print(\"âœ… Separation successful!\\n\")\n",
        "\n",
        "    # Audio Player\n",
        "    print(\"ğŸ§ Original Mixture:\")\n",
        "    ipd.display(ipd.Audio(path_mix))\n",
        "\n",
        "    print(\"ğŸ—£ï¸ Speaker 1 (Est):\")\n",
        "    ipd.display(ipd.Audio(path_s1))\n",
        "\n",
        "    print(\"ğŸ—£ï¸ Speaker 2 (Est):\")\n",
        "    ipd.display(ipd.Audio(path_s2))\n",
        "\n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    def plot_spec(ax, p, title):\n",
        "        y, sr = librosa.load(p, sr=8000)\n",
        "        D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
        "        img = librosa.display.specshow(D, y_axis='linear', x_axis='time', sr=sr, ax=ax)\n",
        "        ax.set_title(title)\n",
        "        return img\n",
        "\n",
        "    img = plot_spec(ax[0], path_mix, \"Mixture\")\n",
        "    plot_spec(ax[1], path_s1, \"Speaker 1\")\n",
        "    plot_spec(ax[2], path_s2, \"Speaker 2\")\n",
        "    fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Output files not found. Check if the model saved to a different location.\")"
      ],
      "metadata": {
        "id": "XQs_szti7zZd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "65beb0ce-70eb-4f7c-d913-507ce149af78"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2515261112.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mipd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/librosa/display.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrename_kw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParameterError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lazy_loader/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{package_name}.{name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattr_to_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0msubmod_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{package_name}.{attr_to_modules[name]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/librosa/util/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlazy_loader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0m__getattr__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach_stub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lazy_loader/__init__.py\u001b[0m in \u001b[0;36mattach_stub\u001b[0;34m(package_name, filename)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstubfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mstub_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0mvisitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_StubVisitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ast.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, filename, mode, type_comments, feature_version)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mfeature_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Else it should be an int giving the minor version for 3.x.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     return compile(source, filename, mode, flags,\n\u001b[0m\u001b[1;32m     53\u001b[0m                    _feature_version=feature_version)\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **save model offline**"
      ],
      "metadata": {
        "id": "r5zbBMC6tOSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "# 1. Define Paths\n",
        "# Source directories\n",
        "model_root = \"/content/SepReformer/models/SepReformer_Base_WSJ0\"\n",
        "weights_dir = os.path.join(model_root, \"log\", \"scratch_weights\")\n",
        "\n",
        "# Destination directory in Drive (Persistent storage)\n",
        "dest_dir = \"/content/drive/MyDrive/×ª×•××¨/speech/SepReformer_Saved_Model\"\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "# 2. Identify Files to Save\n",
        "# A. Latest Checkpoint\n",
        "checkpoints = sorted(glob.glob(os.path.join(weights_dir, \"epoch.*.pth\")))\n",
        "if not checkpoints:\n",
        "    print(\"âŒ No checkpoints found!\")\n",
        "else:\n",
        "    latest_ckpt = checkpoints[-1]\n",
        "    ckpt_name = os.path.basename(latest_ckpt)\n",
        "\n",
        "    # B. Config and Patched Code\n",
        "    files_to_save = {\n",
        "        latest_ckpt: os.path.join(dest_dir, ckpt_name),\n",
        "        os.path.join(model_root, \"configs.yaml\"): os.path.join(dest_dir, \"configs.yaml\"),\n",
        "        os.path.join(model_root, \"dataset.py\"): os.path.join(dest_dir, \"dataset.py\"), # Save patched file\n",
        "        os.path.join(model_root, \"engine.py\"): os.path.join(dest_dir, \"engine.py\")   # Save patched file\n",
        "    }\n",
        "\n",
        "    print(f\"ğŸ’¾ Saving files to: {dest_dir} ...\")\n",
        "\n",
        "    # 3. Copy Files\n",
        "    for src, dst in files_to_save.items():\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy2(src, dst)\n",
        "            print(f\"âœ… Saved: {os.path.basename(src)}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ Warning: Source file not found: {src}\")\n"
      ],
      "metadata": {
        "id": "0icg8CPS8lfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **load model**"
      ],
      "metadata": {
        "id": "QXR9f0m6tUbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Mount Drive ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 2. Setup Repository (Clone & Install) ---\n",
        "if not os.path.exists('/content/SepReformer'):\n",
        "    print(\"ğŸš€ Cloning SepReformer...\")\n",
        "    !git clone https://github.com/dmlguq456/SepReformer.git\n",
        "    %cd SepReformer\n",
        "    print(\"ğŸ“¦ Installing dependencies...\")\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install loguru torchinfo ptflops thop mir_eval\n",
        "else:\n",
        "    print(\"âœ… Repository already exists.\")\n",
        "    %cd /content/SepReformer\n",
        "\n",
        "# --- 3. Restore Saved Model ---\n",
        "# Paths\n",
        "saved_models_dir = '/content/drive/MyDrive/×ª×•××¨/speech/SepReformer_Saved_Model'\n",
        "model_root = 'models/SepReformer_Base_WSJ0' # Relative to /content/SepReformer\n",
        "\n",
        "print(f\"ğŸ”„ Restoring model files from: {saved_models_dir}\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(os.path.join(model_root, 'log', 'scratch_weights'), exist_ok=True)\n",
        "\n",
        "# Files to restore\n",
        "files_to_copy = ['configs.yaml', 'dataset.py', 'engine.py']\n",
        "\n",
        "for fname in files_to_copy:\n",
        "    src = os.path.join(saved_models_dir, fname)\n",
        "    dst = os.path.join(model_root, fname)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy2(src, dst)\n",
        "        print(f\"  - Restored {fname}\")\n",
        "    else:\n",
        "        print(f\"  âš ï¸ Missing {fname} in saved dir\")\n",
        "\n",
        "# Restore Checkpoint\n",
        "saved_ckpts = sorted(glob.glob(os.path.join(saved_models_dir, \"epoch.*.pth\")))\n",
        "if saved_ckpts:\n",
        "    latest_ckpt = saved_ckpts[-1]\n",
        "    ckpt_name = os.path.basename(latest_ckpt)\n",
        "    dst_ckpt = os.path.join(model_root, 'log', 'scratch_weights', ckpt_name)\n",
        "    shutil.copy2(latest_ckpt, dst_ckpt)\n",
        "    print(f\"  - Restored Checkpoint: {ckpt_name}\")\n",
        "else:\n",
        "    print(\"  âŒ No checkpoint found to restore!\")\n",
        "\n",
        "print(\"\\nğŸ‰ Ready! The environment is set up and the model is loaded.\")"
      ],
      "metadata": {
        "id": "LkGZAzLzrSiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Attention model**"
      ],
      "metadata": {
        "id": "M4Y5K8e3tacE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ks4np44ktw0D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}