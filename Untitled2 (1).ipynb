{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1yhfT8RWjSWo4ynA3RZm-va2omGH4u2Jd",
      "authorship_tag": "ABX9TyOdDBoQT6Ga64lhOvu3WoU4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Drive - Data Set**"
      ],
      "metadata": {
        "id": "Zp0G3vhz8_m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8zMdGFmNEmq",
        "outputId": "42d9fc17-591e-4aba-f4cf-9034b7d5ca1a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Clone Paper Code**"
      ],
      "metadata": {
        "id": "yywjDIOO9Nn4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot8kIH-cIbM8",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git lfs install\n",
        "!git clone https://github.com/dmlguq456/SepReformer.git\n",
        "%cd SepReformer\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -r requirements.txt\n",
        "# Note: The repo recommends Python 3.10, which is currently the Colab standard."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "# Configuration\n",
        "base_path = '/content/drive/MyDrive/×ª×•××¨/speech'\n",
        "subfolders = ['s1', 's2', 'mix_clean']\n",
        "scp_output_dir = '/content/SepReformer/data/custom'\n",
        "os.makedirs(scp_output_dir, exist_ok=True)\n",
        "\n",
        "# 1. Get list of file names (assuming they are identical across s1, s2, and mix)\n",
        "# We sort them first to ensure indices match across all subfolders\n",
        "files = sorted([f for f in os.listdir(os.path.join(base_path, 's1')) if f.endswith('.wav')])\n",
        "\n",
        "# 2. Shuffle and Split (80/20)\n",
        "random.seed(42) # For reproducibility\n",
        "random.shuffle(files)\n",
        "split_idx = int(len(files) * 0.8)\n",
        "\n",
        "train_files = files[:split_idx]\n",
        "val_files = files[split_idx:]\n",
        "\n",
        "# 3. Generate the SCP files\n",
        "def create_scp(file_list, set_name):\n",
        "    for folder in subfolders:\n",
        "        output_path = os.path.join(scp_output_dir, f\"{set_name}_{folder}.scp\")\n",
        "        with open(output_path, 'w') as f:\n",
        "            for filename in file_list:\n",
        "                full_path = os.path.join(base_path, folder, filename)\n",
        "                # ID format: filename (must be unique)\n",
        "                f.write(f\"{filename} {full_path}\\n\")\n",
        "        print(f\"Created {output_path} with {len(file_list)} files.\")\n",
        "\n",
        "create_scp(train_files, \"train\")\n",
        "create_scp(val_files, \"val\")"
      ],
      "metadata": {
        "id": "QItPvTylKWXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install loguru torchinfo ptflops thop mir_eval"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Zd903Ld6t9SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_content = \"\"\"\n",
        "project: \"[Project] SepReformer\"\n",
        "notes: \"Training on custom dataset - Fixed Scheduler\"\n",
        "\n",
        "config:\n",
        "    dataset:\n",
        "        max_len: 32000\n",
        "        sampling_rate: 8000\n",
        "        scp_dir: \"data/custom\"\n",
        "        train:\n",
        "            mixture: \"train_mix_clean.scp\"\n",
        "            spk1: \"train_s1.scp\"\n",
        "            spk2: \"train_s2.scp\"\n",
        "            dynamic_mixing: true\n",
        "        valid:\n",
        "            mixture: \"val_mix_clean.scp\"\n",
        "            spk1: \"val_s1.scp\"\n",
        "            spk2: \"val_s2.scp\"\n",
        "        test:\n",
        "            mixture: \"val_mix_clean.scp\"\n",
        "            spk1: \"val_s1.scp\"\n",
        "            spk2: \"val_s2.scp\"\n",
        "\n",
        "    dataloader:\n",
        "        batch_size: 1\n",
        "        pin_memory: true\n",
        "        num_workers: 4\n",
        "        drop_last: true\n",
        "\n",
        "    model:\n",
        "        num_stages: 4\n",
        "        num_spks: 2\n",
        "        module_audio_enc:\n",
        "            in_channels: 1\n",
        "            out_channels: 256\n",
        "            kernel_size: 16\n",
        "            stride: 4\n",
        "            groups: 1\n",
        "            bias: false\n",
        "        module_feature_projector:\n",
        "            num_channels: 256\n",
        "            in_channels: 256\n",
        "            out_channels: 128\n",
        "            kernel_size: 1\n",
        "            bias: false\n",
        "        module_separator:\n",
        "            num_stages: 4\n",
        "            relative_positional_encoding:\n",
        "                in_channels: 128\n",
        "                num_heads: 8\n",
        "                maxlen: 2000\n",
        "                embed_v: false\n",
        "            enc_stage:\n",
        "                global_blocks:\n",
        "                    in_channels: 128\n",
        "                    num_mha_heads: 8\n",
        "                    dropout_rate: 0.05\n",
        "                local_blocks:\n",
        "                    in_channels: 128\n",
        "                    kernel_size: 65\n",
        "                    dropout_rate: 0.05\n",
        "                down_conv_layer:\n",
        "                    in_channels: 128\n",
        "                    samp_kernel_size: 5\n",
        "            spk_split_stage:\n",
        "                in_channels: 128\n",
        "                num_spks: 2\n",
        "            simple_fusion:\n",
        "                out_channels: 128\n",
        "            dec_stage:\n",
        "                num_spks: 2\n",
        "                global_blocks:\n",
        "                    in_channels: 128\n",
        "                    num_mha_heads: 8\n",
        "                    dropout_rate: 0.05\n",
        "                local_blocks:\n",
        "                    in_channels: 128\n",
        "                    kernel_size: 65\n",
        "                    dropout_rate: 0.05\n",
        "                spk_attention:\n",
        "                    in_channels: 128\n",
        "                    num_mha_heads: 8\n",
        "                    dropout_rate: 0.05\n",
        "        module_output_layer:\n",
        "            in_channels: 256\n",
        "            out_channels: 128\n",
        "            num_spks: 2\n",
        "        module_audio_dec:\n",
        "            in_channels: 256\n",
        "            out_channels: 1\n",
        "            kernel_size: 16\n",
        "            stride: 4\n",
        "            bias: false\n",
        "\n",
        "    criterion:\n",
        "        name: [\"PIT_SISNR_mag\", \"PIT_SISNR_time\", \"PIT_SISNRi\", \"PIT_SDRi\"]\n",
        "        PIT_SISNR_mag:\n",
        "            frame_length: 512\n",
        "            frame_shift: 128\n",
        "            window: 'hann'\n",
        "            num_stages: 4\n",
        "            num_spks: 2\n",
        "            scale_inv: true\n",
        "            mel_opt: false\n",
        "        PIT_SISNR_time:\n",
        "            num_spks: 2\n",
        "            scale_inv: true\n",
        "        PIT_SISNRi:\n",
        "            num_spks: 2\n",
        "            scale_inv: true\n",
        "        PIT_SDRi:\n",
        "            dump: 0\n",
        "\n",
        "    optimizer:\n",
        "        name: [\"AdamW\"]\n",
        "        AdamW:\n",
        "            lr: 1.0e-3\n",
        "            weight_decay: 1.0e-2\n",
        "\n",
        "    scheduler:\n",
        "        # Restored \"WarmupConstantSchedule\" here\n",
        "        name: [\"ReduceLROnPlateau\", \"WarmupConstantSchedule\"]\n",
        "        ReduceLROnPlateau:\n",
        "            mode: \"min\"\n",
        "            min_lr: 1.0e-10\n",
        "            factor: 0.8\n",
        "            patience: 2\n",
        "        WarmupConstantSchedule:\n",
        "            warmup_steps: 100\n",
        "\n",
        "    check_computations:\n",
        "        dummy_len: 16000\n",
        "\n",
        "    engine:\n",
        "        max_epoch: 90\n",
        "        gpuid: \"0\"\n",
        "        mvn: false\n",
        "        clip_norm: 5\n",
        "        start_scheduling: 10\n",
        "        test_epochs: [50, 90]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"models/SepReformer_Base_WSJ0/configs.yaml\", \"w\") as f:\n",
        "    f.write(config_content)\n",
        "print(\"Config file updated with restored Scheduler settings!\")\n",
        "!rm -rf models/SepReformer_Base_WSJ0/log\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "import os\n",
        "\n",
        "# Path to the file\n",
        "engine_path = '/content/SepReformer/models/SepReformer_Base_WSJ0/engine.py'\n",
        "\n",
        "# Read the file\n",
        "with open(engine_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Fix the specific line\n",
        "new_lines = []\n",
        "found = False\n",
        "for line in lines:\n",
        "    # Look for the save_checkpoint_per_best call\n",
        "    if 'util_engine.save_checkpoint_per_best' in line:\n",
        "        # We need to remove the last argument (wandb_run or getattr...)\n",
        "        # The correct call should end after 'self.checkpoint_path'\n",
        "        # Let's construct the correct line based on the error message.\n",
        "        # It takes 7 args: valid_loss_best, val_loss, train_loss, epoch, model, optimizer, path\n",
        "\n",
        "        indent = line.split(\"valid_loss_best\")[0] # Preserve indentation\n",
        "        new_line = (\n",
        "            f\"{indent}valid_loss_best = util_engine.save_checkpoint_per_best(\"\n",
        "            f\"valid_loss_best, valid_loss_src_time, train_loss_src_time, epoch, \"\n",
        "            f\"self.model, self.main_optimizer, self.checkpoint_path)\\n\"\n",
        "        )\n",
        "        new_lines.append(new_line)\n",
        "        found = True\n",
        "        print(\"Fixed line to 7 arguments.\")\n",
        "    else:\n",
        "        new_lines.append(line)\n",
        "\n",
        "if found:\n",
        "    with open(engine_path, 'w') as f:\n",
        "        f.writelines(new_lines)\n",
        "    print(\"Successfully patched engine.py!\")\n",
        "else:\n",
        "    print(\"Could not find the specific line to patch. Please check file content.\")\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "import os\n",
        "\n",
        "# ×”× ×ª×™×‘ ×œ×§×•×‘×¥\n",
        "engine_path = '/content/SepReformer/models/SepReformer_Base_WSJ0/engine.py'\n",
        "\n",
        "# ×§×¨×™××ª ×”×§×•×‘×¥\n",
        "with open(engine_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# ×ª×™×§×•×Ÿ ×”×©×•×¨×”\n",
        "new_lines = []\n",
        "found = False\n",
        "for line in lines:\n",
        "    # ××—×¤×©×™× ××ª ×”×©×•×¨×” ×©×× ×¡×” ×œ×¨×©×•× ××ª ×”-Learning Rate\n",
        "    if 'writer_src.add_scalars(\"Learning Rate\"' in line:\n",
        "        # ××—×œ×™×¤×™× ××ª add_scalars ×‘-add_scalar (×‘×™×—×™×“)\n",
        "        new_line = line.replace('add_scalars', 'add_scalar')\n",
        "        new_lines.append(new_line)\n",
        "        found = True\n",
        "        print(\"Fixed line:\", new_line.strip())\n",
        "    else:\n",
        "        new_lines.append(line)\n",
        "\n",
        "if found:\n",
        "    with open(engine_path, 'w') as f:\n",
        "        f.writelines(new_lines)\n",
        "    print(\"Successfully patched engine.py (add_scalar fix)!\")\n",
        "else:\n",
        "    print(\"Could not find the specific line to patch.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myoGETBJynof",
        "outputId": "b540c0dd-c0ad-462c-d383-81c0fe3aa525"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config file updated with restored Scheduler settings!\n",
            "Fixed line to 7 arguments.\n",
            "Successfully patched engine.py!\n",
            "Fixed line: writer_src.add_scalar(\"Learning Rate\", self.main_optimizer.param_groups[0]['lr'], epoch)\n",
            "Successfully patched engine.py (add_scalar fix)!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# control **epochs**"
      ],
      "metadata": {
        "id": "TWswwwKBtn14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "config_path = 'models/SepReformer_Base_WSJ0/configs.yaml'\n",
        "\n",
        "# ×§×¨×™××ª ×”×§×•×‘×¥\n",
        "with open(config_path, 'r') as f:\n",
        "    # × ×˜×¢×Ÿ ××ª ×”×§×•×‘×¥ ×›×˜×§×¡×˜ ×¤×©×•×˜ ×›×“×™ ×œ× ×œ×©×‘×•×¨ ××ª ×”×”×¢×¨×•×ª ×•×”××‘× ×” ×”××™×•×—×“\n",
        "    content = f.read()\n",
        "\n",
        "# ×‘×™×¦×•×¢ ×”×—×œ×¤×•×ª ×¤×©×•×˜×•×ª ×œ×©×™× ×•×™ ××¡×¤×¨ ×”-Epochs\n",
        "content = content.replace('max_epoch: 100', 'max_epoch: 6')\n",
        "content = content.replace('test_epochs: [50, 100]', 'test_epochs: [6]')\n",
        "\n",
        "# ×©××™×¨×” ××—×“×©\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"Updated config: max_epoch = 6, test_epochs = [6]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaFTnm1Q38i4",
        "outputId": "f8d54a67-994c-48c0-fc08-cdc6925c2fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated config: max_epoch = 6, test_epochs = [6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ×”× ×ª×™×‘ ×œ×§×•×‘×¥ ×”×‘×¢×™×™×ª×™\n",
        "dataset_path = '/content/SepReformer/models/SepReformer_Base_WSJ0/dataset.py'\n",
        "\n",
        "# ×§×¨×™××ª ×”×§×•×‘×¥\n",
        "with open(dataset_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# ×ª×™×§×•×Ÿ ×”×œ×•×’×™×§×”\n",
        "new_lines = []\n",
        "found = False\n",
        "for line in lines:\n",
        "    # ××—×¤×©×™× ××ª ×”×©×•×¨×” ×©×× ×¡×” ×œ×¤×¨×§ ××ª ×©××•×ª ×”×§×‘×¦×™× ×¢× split('_')\n",
        "    # ×”×©×•×¨×” ×”××§×•×¨×™×ª × ×¨××™×ª ×‘×¢×¨×š ×›×š:\n",
        "    # tmp1 = key.split('_')[1][:3] != key_random.split('_')[3][:3]\n",
        "    if \"key.split('_')\" in line and \"key_random.split('_')\" in line:\n",
        "        # ××—×œ×™×¤×™× ×‘×œ×•×’×™×§×” ×¤×©×•×˜×” ×©×ª××™×“ ××—×–×™×¨×” True (×××¤×©×¨×ª ×¢×¨×‘×•×‘)\n",
        "        # ×©×•××¨×™× ×¢×œ ×”×”×–×—×” (Indentation) ×”××§×•×¨×™×ª\n",
        "        indent = line[:line.find(\"tmp1\")]\n",
        "        new_line = f\"{indent}tmp1 = True # Patched for custom filenames\\n\"\n",
        "        new_lines.append(new_line)\n",
        "\n",
        "        # ×× ×—× ×• ×¦×¨×™×›×™× ×’× ×œ× ×˜×¨×œ ××ª ×”×©×•×¨×•×ª ×”×‘××•×ª ×©×‘×•×“×§×•×ª ××ª tmp2 ×•××ª ×”×ª× ××™ ×”××•×¨×›×‘\n",
        "        # ××‘×œ ×”×“×¨×š ×”×›×™ ×§×œ×” ×”×™× ×œ×ª×ª ×œ-tmp1 ×œ×”×™×•×ª True, ×•×œ×©× ×•×ª ×’× ××ª tmp2 ×× ×¦×¨×™×š.\n",
        "        # ×œ××¢×©×”, ×”×¤×ª×¨×•×Ÿ ×”×›×™ ×™×¦×™×‘ ×”×•× ×œ×¢×˜×•×£ ××ª ×›×œ ×”×‘×œ×•×§ ×‘-try/except, ××‘×œ ×¢×¨×™×›×” ×›×–×• ××¡×•×‘×›×ª ×‘×¡×§×¨×™×¤×˜.\n",
        "        # ×”×¤×ª×¨×•×Ÿ ×”×¤×©×•×˜: × ×”×¤×•×š ××ª ×›×œ ×‘×“×™×§×•×ª ×”×“×•×‘×¨×™× ×œ-True.\n",
        "        found = True\n",
        "        print(\"Fixed tmp1 check.\")\n",
        "\n",
        "    elif \"tmp2 =\" in line and \"key.split\" in line:\n",
        "         indent = line[:line.find(\"tmp2\")]\n",
        "         new_lines.append(f\"{indent}tmp2 = True # Patched\\n\")\n",
        "         print(\"Fixed tmp2 check.\")\n",
        "\n",
        "    else:\n",
        "        new_lines.append(line)\n",
        "\n",
        "if found:\n",
        "    with open(dataset_path, 'w') as f:\n",
        "        f.writelines(new_lines)\n",
        "    print(\"âœ… Successfully patched dataset.py to support custom filenames!\")\n",
        "else:\n",
        "    print(\"âŒ Could not find the lines to patch. Please check file content manually.\")\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "file_path = '/content/SepReformer/models/SepReformer_Base_WSJ0/dataset.py'\n",
        "\n",
        "print(f\"ğŸ”§ Fixing {file_path}...\")\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "new_lines = []\n",
        "fixed = False\n",
        "\n",
        "for line in lines:\n",
        "    # ×–×™×”×•×™ ×”×©×•×¨×” ×”××©×•×‘×©×ª (×©××›×™×œ×” ×’× tmp2 ×•×’× tmp1 ×‘×™×—×“)\n",
        "    if \"tmp2 =\" in line and \"tmp1 =\" in line:\n",
        "        # ×©×•××¨×™× ×¢×œ ×”×”×–×—×” (Indentation) ×”××§×•×¨×™×ª\n",
        "        indent = line[:line.find(\"tmp2\")]\n",
        "        # ×›×•×ª×‘×™× ××ª ×”×¤×§×•×“×•×ª ×‘×©×•×¨×•×ª × ×¤×¨×“×•×ª\n",
        "        new_lines.append(f\"{indent}tmp1 = True # Fixed\\n\")\n",
        "        new_lines.append(f\"{indent}tmp2 = True # Fixed\\n\")\n",
        "        fixed = True\n",
        "        print(\"âœ… Found and fixed the broken line!\")\n",
        "\n",
        "    # ×˜×™×¤×•×œ ×‘××§×¨×™× ×©×‘×”× ×”×©×•×¨×•×ª ×¢×“×™×™×Ÿ ××§×•×¨×™×•×ª ××š ×œ× ××ª××™××•×ª (×œ×× ×™×¢×ª ×‘×¢×™×•×ª ×¢×ª×™×“×™×•×ª)\n",
        "    elif \"tmp1 =\" in line and \"key.split\" in line:\n",
        "        indent = line[:line.find(\"tmp1\")]\n",
        "        new_lines.append(f\"{indent}tmp1 = True # Patched\\n\")\n",
        "        fixed = True\n",
        "    elif \"tmp2 =\" in line and \"key.split\" in line:\n",
        "        indent = line[:line.find(\"tmp2\")]\n",
        "        new_lines.append(f\"{indent}tmp2 = True # Patched\\n\")\n",
        "        fixed = True\n",
        "\n",
        "    else:\n",
        "        new_lines.append(line)\n",
        "\n",
        "if fixed:\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.writelines(new_lines)\n",
        "    print(\"ğŸš€ File updated successfully.\")\n",
        "else:\n",
        "    print(\"âš ï¸ No broken lines found. The file might be already fixed or different than expected.\")\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# ×”× ×ª×™×‘ ×œ×§×•×‘×¥ ×”×‘×¢×™×™×ª×™\n",
        "dataset_path = '/content/SepReformer/models/SepReformer_Base_WSJ0/dataset.py'\n",
        "\n",
        "# ×§×¨×™××ª ×”×§×•×‘×¥\n",
        "with open(dataset_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# ×ª×™×§×•×Ÿ ×”×©×•×¨×”\n",
        "new_lines = []\n",
        "found = False\n",
        "for line in lines:\n",
        "    # ××—×¤×©×™× ××ª ×”×©×•×¨×” ×©××‘×¦×¢×ª speed_aug\n",
        "    if 'self.speed_aug(torch.tensor(samps_tmp))' in line:\n",
        "        # ×¤×©×•×˜ × ×“×œ×’ ×¢×œ×™×” ××• × ×”×¤×•×š ××•×ª×” ×œ×”×¢×¨×”.\n",
        "        # ×‘××§×•× ×œ××—×•×§, × ×©××™×¨ ××ª samps_tmp ×›××• ×©×”×•×.\n",
        "        # ×”×©×•×¨×” ×”××§×•×¨×™×ª: samps_tmp = np.array(self.speed_aug(torch.tensor(samps_tmp))[0])\n",
        "        # ×”×—×“×©×”: # samps_tmp = ... (skipped)\n",
        "        new_lines.append(f\"            # {line.strip()} # PATCHED: Disabled speed_aug\\n\")\n",
        "        found = True\n",
        "        print(\"Disabled speed_aug line.\")\n",
        "    else:\n",
        "        new_lines.append(line)\n",
        "\n",
        "if found:\n",
        "    with open(dataset_path, 'w') as f:\n",
        "        f.writelines(new_lines)\n",
        "    print(\"âœ… Successfully patched dataset.py to remove speed augmentation bug!\")\n",
        "else:\n",
        "    print(\"âš ï¸ Could not find the speed_aug line. It might be already fixed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK6KafFeBPzL",
        "outputId": "e0ac7458-2e92-4211-f88a-396b91daac38"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed tmp1 check.\n",
            "Fixed tmp1 check.\n",
            "âœ… Successfully patched dataset.py to support custom filenames!\n",
            "ğŸ”§ Fixing /content/SepReformer/models/SepReformer_Base_WSJ0/dataset.py...\n",
            "âœ… Found and fixed the broken line!\n",
            "ğŸš€ File updated successfully.\n",
            "Disabled speed_aug line.\n",
            "âœ… Successfully patched dataset.py to remove speed augmentation bug!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train**"
      ],
      "metadata": {
        "id": "z1AJZFXV1LIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run.py --model SepReformer_Base_WSJ0 --engine-mode train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKmP1wIUv8ux",
        "outputId": "4f009ad9-bcd2-47da-ff4d-60a4a9052d07",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-15 19:20:05.475002: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-02-15 19:20:05.493176: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771183205.515979    9979 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771183205.523435    9979 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771183205.542759    9979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771183205.542790    9979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771183205.542793    9979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771183205.542796    9979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-15 19:20:05.547758: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m2026-02-15 19:20:11.541\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mEntering 'main' (args=(Namespace(model='SepReformer_Base_WSJ0', engine_mode='train', sample_file=None, out_wav_dir=None),), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.542\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m20\u001b[0m - \u001b[34m\u001b[1mEntering 'parse_yaml' (args=('/content/SepReformer/models/SepReformer_Base_WSJ0/configs.yaml',), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.551\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m20\u001b[0m - \u001b[34m\u001b[1mExiting 'parse_yaml' (result={'project': '[Project] SepReformer', 'notes': 'Training on custom dataset - Fixed Scheduler', 'config': {'dataset': {'max_len': 32000, 'sampling_rate': 8000, 'scp_dir': 'data/custom', 'train': {'mixture': 'train_mix_clean.scp', 'spk1': 'train_s1.scp', 'spk2': 'train_s2.scp', 'dynamic_mixing': True}, 'valid': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}, 'test': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}}, 'dataloader': {'batch_size': 1, 'pin_memory': True, 'num_workers': 4, 'drop_last': True}, 'model': {'num_stages': 4, 'num_spks': 2, 'module_audio_enc': {'in_channels': 1, 'out_channels': 256, 'kernel_size': 16, 'stride': 4, 'groups': 1, 'bias': False}, 'module_feature_projector': {'num_channels': 256, 'in_channels': 256, 'out_channels': 128, 'kernel_size': 1, 'bias': False}, 'module_separator': {'num_stages': 4, 'relative_positional_encoding': {'in_channels': 128, 'num_heads': 8, 'maxlen': 2000, 'embed_v': False}, 'enc_stage': {'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'down_conv_layer': {'in_channels': 128, 'samp_kernel_size': 5}}, 'spk_split_stage': {'in_channels': 128, 'num_spks': 2}, 'simple_fusion': {'out_channels': 128}, 'dec_stage': {'num_spks': 2, 'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'spk_attention': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}}}, 'module_output_layer': {'in_channels': 256, 'out_channels': 128, 'num_spks': 2}, 'module_audio_dec': {'in_channels': 256, 'out_channels': 1, 'kernel_size': 16, 'stride': 4, 'bias': False}}, 'criterion': {'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, 'optimizer': {'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, 'scheduler': {'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, 'check_computations': {'dummy_len': 16000}, 'engine': {'max_epoch': 90, 'gpuid': '0', 'mvn': False, 'clip_norm': 5, 'start_scheduling': 10, 'test_epochs': [50, 90]}}})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.551\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m26\u001b[0m - \u001b[34m\u001b[1mEntering 'get_dataloaders' (args=(Namespace(model='SepReformer_Base_WSJ0', engine_mode='train', sample_file=None, out_wav_dir=None), {'max_len': 32000, 'sampling_rate': 8000, 'scp_dir': 'data/custom', 'train': {'mixture': 'train_mix_clean.scp', 'spk1': 'train_s1.scp', 'spk2': 'train_s2.scp', 'dynamic_mixing': True}, 'valid': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}, 'test': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}}, {'batch_size': 1, 'pin_memory': True, 'num_workers': 4, 'drop_last': True}), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.551\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mEntering 'MyDataset' (args=(), kwargs={'max_len': 32000, 'fs': 8000, 'partition': 'train', 'wave_scp_srcs': ['data/custom/train_s1.scp', 'data/custom/train_s2.scp'], 'wave_scp_mix': 'data/custom/train_mix_clean.scp', 'dynamic_mixing': True})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mCreate MyDataset for data/custom/train_mix_clean.scp with 160 utterances\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.552\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mExiting 'MyDataset' (result=<models.SepReformer_Base_WSJ0.dataset.MyDataset object at 0x7b18e02442c0>)\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.552\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mEntering 'MyDataset' (args=(), kwargs={'max_len': 32000, 'fs': 8000, 'partition': 'valid', 'wave_scp_srcs': ['data/custom/val_s1.scp', 'data/custom/val_s2.scp'], 'wave_scp_mix': 'data/custom/val_mix_clean.scp', 'dynamic_mixing': False})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mCreate MyDataset for data/custom/val_mix_clean.scp with 40 utterances\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.553\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mExiting 'MyDataset' (result=<models.SepReformer_Base_WSJ0.dataset.MyDataset object at 0x7b18e02440b0>)\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.553\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mEntering 'MyDataset' (args=(), kwargs={'max_len': 32000, 'fs': 8000, 'partition': 'test', 'wave_scp_srcs': ['data/custom/val_s1.scp', 'data/custom/val_s2.scp'], 'wave_scp_mix': 'data/custom/val_mix_clean.scp', 'dynamic_mixing': False})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mCreate MyDataset for data/custom/val_mix_clean.scp with 40 utterances\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.553\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mExiting 'MyDataset' (result=<models.SepReformer_Base_WSJ0.dataset.MyDataset object at 0x7b18e0244110>)\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.553\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m26\u001b[0m - \u001b[34m\u001b[1mExiting 'get_dataloaders' (result={'train': <torch.utils.data.dataloader.DataLoader object at 0x7b1b22a444a0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7b18e0244050>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7b18e0244230>})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.554\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mEntering 'Model' (args=(), kwargs={'num_stages': 4, 'num_spks': 2, 'module_audio_enc': {'in_channels': 1, 'out_channels': 256, 'kernel_size': 16, 'stride': 4, 'groups': 1, 'bias': False}, 'module_feature_projector': {'num_channels': 256, 'in_channels': 256, 'out_channels': 128, 'kernel_size': 1, 'bias': False}, 'module_separator': {'num_stages': 4, 'relative_positional_encoding': {'in_channels': 128, 'num_heads': 8, 'maxlen': 2000, 'embed_v': False}, 'enc_stage': {'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'down_conv_layer': {'in_channels': 128, 'samp_kernel_size': 5}}, 'spk_split_stage': {'in_channels': 128, 'num_spks': 2}, 'simple_fusion': {'out_channels': 128}, 'dec_stage': {'num_spks': 2, 'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'spk_attention': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}}}, 'module_output_layer': {'in_channels': 256, 'out_channels': 128, 'num_spks': 2}, 'module_audio_dec': {'in_channels': 256, 'out_channels': 1, 'kernel_size': 16, 'stride': 4, 'bias': False}})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.799\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mExiting 'Model' (result=Model(\n",
            "  (audio_encoder): AudioEncoder(\n",
            "    (conv1d): Conv1d(1, 256, kernel_size=(16,), stride=(4,), bias=False)\n",
            "    (gelu): GELU(approximate='none')\n",
            "  )\n",
            "  (feature_projector): FeatureProjector(\n",
            "    (norm): GroupNorm(1, 256, eps=1e-08, affine=True)\n",
            "    (conv1d): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "  )\n",
            "  (separator): Separator(\n",
            "    (pos_emb): RelativePositionalEncoding(\n",
            "      (pe_k): Embedding(4000, 16)\n",
            "    )\n",
            "    (enc_stages): ModuleList(\n",
            "      (0-3): 4 x SepEncStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downconv): DownConvLayer(\n",
            "          (down_conv): Conv1d(128, 128, kernel_size=(5,), stride=(2,), padding=(2,), groups=128)\n",
            "          (BN): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (gelu): GELU(approximate='none')\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (bottleneck_G): SepEncStage(\n",
            "      (g_block_1): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_1): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (g_block_2): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_2): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (spk_split_block): SpkSplitStage(\n",
            "      (mha): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.05, inplace=False)\n",
            "    )\n",
            "    (simple_fusion): ModuleList(\n",
            "      (0-3): 4 x Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "    (dec_stages): ModuleList(\n",
            "      (0-3): 4 x SepDecStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_1): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_2): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_3): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_3): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_3): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (out_layer): OutputLayer(\n",
            "    (spe_block): Masking(\n",
            "      (gate_act): ReLU()\n",
            "    )\n",
            "    (end_conv1x1): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (1): GLU(dim=-1)\n",
            "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (audio_decoder): AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  (out_layer_bn): ModuleList(\n",
            "    (0-3): 4 x OutputLayer(\n",
            "      (spe_block): Masking(\n",
            "        (gate_act): ReLU()\n",
            "      )\n",
            "      (end_conv1x1): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): GLU(dim=-1)\n",
            "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder_bn): ModuleList(\n",
            "    (0-3): 4 x AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  )\n",
            "))\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.812\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mEntering 'CriterionFactory' (args=({'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, device(type='cuda', index=0)), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:11.812\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mExiting 'CriterionFactory' (result=CriterionFactory(config={'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, device=device(type='cuda', index=0)))\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:12.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SISNR_mag instance with args: {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:12.261\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SISNR_mag' (args=(device(type='cuda', index=0),), kwargs={'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:12.261\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:12.804\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.255\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.257\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.260\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.262\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.265\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.267\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.270\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SISNR_mag' (result=<PIT_SISNR_mag(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', num_stages=4, num_spks=2, scale_inv=True, mel_opt=False, stft = [STFT instance for 4 layers], mel_fb=Identity)>)\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SISNR_time instance with args: {'num_spks': 2, 'scale_inv': True}\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.270\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SISNR_time' (args=(device(type='cuda', index=0),), kwargs={'num_spks': 2, 'scale_inv': True})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.270\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SISNR_time' (result=<PIT_SISNR_time(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>)\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SISNRi instance with args: {'num_spks': 2, 'scale_inv': True}\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.270\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SISNRi' (args=(device(type='cuda', index=0),), kwargs={'num_spks': 2, 'scale_inv': True})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.270\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SISNRi' (result=<PIT_SISNRi(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>)\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SDRi instance with args: {'dump': 0}\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.270\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SDRi' (args=(device(type='cuda', index=0),), kwargs={'dump': 0})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.270\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SDRi' (result=<PIT_SDRi(device=device(type='cuda', index=0), dump=0)>)\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.270\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mEntering 'OptimizerFactory' (args=({'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, <generator object Module.parameters at 0x7b18dfde26c0>), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.271\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mExiting 'OptimizerFactory' (result=OptimizerFactory(config={'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, parameters_policy=<generator object Module.parameters at 0x7b18dfde26c0>))\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating AdamW instance with args: {'lr': 0.001, 'weight_decay': 0.01}\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.275\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m40\u001b[0m - \u001b[34m\u001b[1mEntering 'SchedulerFactory' (args=({'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, [AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")]), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.275\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m40\u001b[0m - \u001b[34m\u001b[1mExiting 'SchedulerFactory' (result=SchedulerFactory(config={'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, optimizers=[AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")]))\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating ReduceLROnPlateau instance with args: {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.277\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating WarmupConstantSchedule instance with args: {'warmup_steps': 100}\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.277\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'WarmupConstantSchedule' (args=(AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "),), kwargs={'warmup_steps': 100})\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.277\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'WarmupConstantSchedule' (result=<WarmupConstantSchedule(optimizer=AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.001\n",
            "    lr: 0.0\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "), warmup_steps=100, last_epoch=0, lr_lambda = <function WarmupConstantSchedule.__post_init__.<locals>.lr_lambda at 0x7b18e02327a0>)>)\u001b[0m\n",
            "\u001b[32m2026-02-15 19:20:13.277\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mEntering 'Engine' (args=(Namespace(model='SepReformer_Base_WSJ0', engine_mode='train', sample_file=None, out_wav_dir=None), {'dataset': {'max_len': 32000, 'sampling_rate': 8000, 'scp_dir': 'data/custom', 'train': {'mixture': 'train_mix_clean.scp', 'spk1': 'train_s1.scp', 'spk2': 'train_s2.scp', 'dynamic_mixing': True}, 'valid': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}, 'test': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}}, 'dataloader': {'batch_size': 1, 'pin_memory': True, 'num_workers': 4, 'drop_last': True}, 'model': {'num_stages': 4, 'num_spks': 2, 'module_audio_enc': {'in_channels': 1, 'out_channels': 256, 'kernel_size': 16, 'stride': 4, 'groups': 1, 'bias': False}, 'module_feature_projector': {'num_channels': 256, 'in_channels': 256, 'out_channels': 128, 'kernel_size': 1, 'bias': False}, 'module_separator': {'num_stages': 4, 'relative_positional_encoding': {'in_channels': 128, 'num_heads': 8, 'maxlen': 2000, 'embed_v': False}, 'enc_stage': {'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'down_conv_layer': {'in_channels': 128, 'samp_kernel_size': 5}}, 'spk_split_stage': {'in_channels': 128, 'num_spks': 2}, 'simple_fusion': {'out_channels': 128}, 'dec_stage': {'num_spks': 2, 'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'spk_attention': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}}}, 'module_output_layer': {'in_channels': 256, 'out_channels': 128, 'num_spks': 2}, 'module_audio_dec': {'in_channels': 256, 'out_channels': 1, 'kernel_size': 16, 'stride': 4, 'bias': False}}, 'criterion': {'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, 'optimizer': {'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, 'scheduler': {'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, 'check_computations': {'dummy_len': 16000}, 'engine': {'max_epoch': 90, 'gpuid': '0', 'mvn': False, 'clip_norm': 5, 'start_scheduling': 10, 'test_epochs': [50, 90]}}, Model(\n",
            "  (audio_encoder): AudioEncoder(\n",
            "    (conv1d): Conv1d(1, 256, kernel_size=(16,), stride=(4,), bias=False)\n",
            "    (gelu): GELU(approximate='none')\n",
            "  )\n",
            "  (feature_projector): FeatureProjector(\n",
            "    (norm): GroupNorm(1, 256, eps=1e-08, affine=True)\n",
            "    (conv1d): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "  )\n",
            "  (separator): Separator(\n",
            "    (pos_emb): RelativePositionalEncoding(\n",
            "      (pe_k): Embedding(4000, 16)\n",
            "    )\n",
            "    (enc_stages): ModuleList(\n",
            "      (0-3): 4 x SepEncStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downconv): DownConvLayer(\n",
            "          (down_conv): Conv1d(128, 128, kernel_size=(5,), stride=(2,), padding=(2,), groups=128)\n",
            "          (BN): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (gelu): GELU(approximate='none')\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (bottleneck_G): SepEncStage(\n",
            "      (g_block_1): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_1): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (g_block_2): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_2): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (spk_split_block): SpkSplitStage(\n",
            "      (mha): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.05, inplace=False)\n",
            "    )\n",
            "    (simple_fusion): ModuleList(\n",
            "      (0-3): 4 x Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "    (dec_stages): ModuleList(\n",
            "      (0-3): 4 x SepDecStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_1): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_2): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_3): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_3): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_3): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (out_layer): OutputLayer(\n",
            "    (spe_block): Masking(\n",
            "      (gate_act): ReLU()\n",
            "    )\n",
            "    (end_conv1x1): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (1): GLU(dim=-1)\n",
            "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (audio_decoder): AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  (out_layer_bn): ModuleList(\n",
            "    (0-3): 4 x OutputLayer(\n",
            "      (spe_block): Masking(\n",
            "        (gate_act): ReLU()\n",
            "      )\n",
            "      (end_conv1x1): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): GLU(dim=-1)\n",
            "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder_bn): ModuleList(\n",
            "    (0-3): 4 x AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  )\n",
            "), {'train': <torch.utils.data.dataloader.DataLoader object at 0x7b1b22a444a0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x7b18e0244050>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7b18e0244230>}, [<PIT_SISNR_mag(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', num_stages=4, num_spks=2, scale_inv=True, mel_opt=False, stft = [STFT instance for 4 layers], mel_fb=Identity)>, <PIT_SISNR_time(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>, <PIT_SISNRi(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>, <PIT_SDRi(device=device(type='cuda', index=0), dump=0)>], [AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.001\n",
            "    lr: 0.0\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")], [<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7b18dfdfb3b0>, <WarmupConstantSchedule(optimizer=AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.001\n",
            "    lr: 0.0\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "), warmup_steps=100, last_epoch=0, lr_lambda = <function WarmupConstantSchedule.__post_init__.<locals>.lr_lambda at 0x7b18e02327a0>)>], (0,), device(type='cuda', index=0)), kwargs={})\u001b[0m\n",
            "Flops estimation was not finished successfully because of the following exception: \n",
            "<class 'ValueError'>: not enough values to unpack (expected 3, got 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ptflops/pytorch_engine.py\", line 68, in get_flops_pytorch\n",
            "    _ = flops_model(batch)\n",
            "        ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1881, in _call_impl\n",
            "    return inner()\n",
            "           ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1829, in inner\n",
            "    result = forward_call(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/model.py\", line 41, in forward\n",
            "    last_stage_output, each_stage_outputs = self.separator(projected_feature)\n",
            "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/modules/module.py\", line 224, in forward\n",
            "    skip_ = self.spk_split_block(skip_)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/modules/module.py\", line 137, in forward\n",
            "    attn_out, _ = self.mha(query=queries, key=keys_values, value=keys_values)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1881, in _call_impl\n",
            "    return inner()\n",
            "           ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1842, in inner\n",
            "    hook_result = hook(self, args, result)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ptflops/pytorch_ops.py\", line 183, in multihead_attention_counter_hook\n",
            "    q, k, v = input\n",
            "    ^^^^^^^\n",
            "ValueError: not enough values to unpack (expected 3, got 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/SepReformer/run.py\", line 32, in <module>\n",
            "    main_module.main(args)\n",
            "  File \"/content/SepReformer/utils/decorators.py\", line 12, in wrapped\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/main.py\", line 43, in main\n",
            "    engine = Engine(args, config, model, dataloaders, criterions, optimizers, schedulers, gpuid, device)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/utils/decorators.py\", line 12, in wrapped\n",
            "    result = func(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SepReformer/models/SepReformer_Base_WSJ0/engine.py\", line 39, in __init__\n",
            "    util_engine.model_params_mac_summary(\n",
            "  File \"/content/SepReformer/utils/util_engine.py\", line 137, in model_params_mac_summary\n",
            "    MACs_ptflops, params_ptflops = MACs_ptflops.replace(\" MMac\", \"\"), params_ptflops.replace(\" M\", \"\")\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'NoneType' object has no attribute 'replace'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l models/SepReformer_Base_WSJ0/log/scratch_weights/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFCTfJ574Eiu",
        "outputId": "ebbade4f-e97a-4f25-b293-e60ead2ea61a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 15509500\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:22 epoch.0001.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:23 epoch.0002.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:24 epoch.0003.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:25 epoch.0004.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:26 epoch.0005.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:27 epoch.0006.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:28 epoch.0007.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:29 epoch.0008.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:30 epoch.0009.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:31 epoch.0010.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:32 epoch.0011.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:33 epoch.0012.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:34 epoch.0013.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:35 epoch.0014.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:36 epoch.0015.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:37 epoch.0016.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:38 epoch.0017.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:39 epoch.0018.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:40 epoch.0019.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:41 epoch.0020.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:42 epoch.0021.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:43 epoch.0022.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:44 epoch.0023.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:45 epoch.0024.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:46 epoch.0025.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:47 epoch.0026.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:48 epoch.0027.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:49 epoch.0028.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:50 epoch.0029.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:51 epoch.0030.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:52 epoch.0031.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:53 epoch.0032.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:54 epoch.0033.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:55 epoch.0034.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:56 epoch.0035.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:57 epoch.0036.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:58 epoch.0037.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 15:59 epoch.0038.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:00 epoch.0039.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:01 epoch.0040.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:02 epoch.0041.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:03 epoch.0042.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:04 epoch.0043.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:05 epoch.0044.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:06 epoch.0045.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:07 epoch.0046.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:07 epoch.0047.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:08 epoch.0048.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:09 epoch.0049.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:11 epoch.0050.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:12 epoch.0051.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:13 epoch.0052.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:14 epoch.0053.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:15 epoch.0054.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:16 epoch.0055.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:17 epoch.0056.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:18 epoch.0057.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:19 epoch.0058.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:20 epoch.0059.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:21 epoch.0060.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:22 epoch.0061.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:23 epoch.0062.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:24 epoch.0063.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:25 epoch.0064.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:26 epoch.0065.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:27 epoch.0066.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:28 epoch.0067.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:29 epoch.0068.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:30 epoch.0069.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:31 epoch.0070.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:32 epoch.0071.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:33 epoch.0072.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:34 epoch.0073.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:35 epoch.0074.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:36 epoch.0075.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:37 epoch.0076.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:38 epoch.0077.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:38 epoch.0078.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:39 epoch.0079.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:40 epoch.0080.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:41 epoch.0081.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:42 epoch.0082.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:43 epoch.0083.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:44 epoch.0084.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:45 epoch.0085.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:46 epoch.0086.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:47 epoch.0087.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:48 epoch.0088.pth\n",
            "-rw-r--r-- 1 root root 178445825 Feb 15 16:49 epoch.0089.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "# ×‘×—×™×¨×ª ×§×•×‘×¥ ××§×¨××™\n",
        "mix_dir = '/content/drive/MyDrive/×ª×•××¨/speech/mix_clean'\n",
        "files = [f for f in os.listdir(mix_dir) if f.endswith('.wav')]\n",
        "test_file = random.choice(files)\n",
        "test_file_path = os.path.join(mix_dir, test_file)\n",
        "\n",
        "print(f\"Testing on: {test_file_path}\")\n",
        "\n",
        "# ×”×¤×§×•×“×” ×”××ª×•×§× ×ª (×©×™× ×œ×‘ ×œ××§×¤×™× ×‘-out-wav-dir)\n",
        "!python run.py \\\n",
        "  --model SepReformer_Base_WSJ0 \\\n",
        "  --engine-mode infer_sample \\\n",
        "  --sample-file \"{test_file_path}\" \\\n",
        "  --out-wav-dir \"results_debug\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st6anh3k695s",
        "outputId": "6455aed2-1472-4582-b9c7-ce3d081241c6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on: /content/drive/MyDrive/×ª×•××¨/speech/mix_clean/mix_00046.wav\n",
            "2026-02-15 17:07:15.779311: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-02-15 17:07:15.798412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1771175235.821118   48215 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1771175235.828691   48215 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1771175235.847995   48215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771175235.848021   48215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771175235.848024   48215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1771175235.848027   48215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-15 17:07:15.852991: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m2026-02-15 17:07:21.457\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mEntering 'main' (args=(Namespace(model='SepReformer_Base_WSJ0', engine_mode='infer_sample', sample_file='/content/drive/MyDrive/×ª×•××¨/speech/mix_clean/mix_00046.wav', out_wav_dir='results_debug'),), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.457\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m20\u001b[0m - \u001b[34m\u001b[1mEntering 'parse_yaml' (args=('/content/SepReformer/models/SepReformer_Base_WSJ0/configs.yaml',), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.466\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m20\u001b[0m - \u001b[34m\u001b[1mExiting 'parse_yaml' (result={'project': '[Project] SepReformer', 'notes': 'Training on custom dataset - Fixed Scheduler', 'config': {'dataset': {'max_len': 32000, 'sampling_rate': 8000, 'scp_dir': 'data/custom', 'train': {'mixture': 'train_mix_clean.scp', 'spk1': 'train_s1.scp', 'spk2': 'train_s2.scp', 'dynamic_mixing': True}, 'valid': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}, 'test': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}}, 'dataloader': {'batch_size': 1, 'pin_memory': True, 'num_workers': 4, 'drop_last': True}, 'model': {'num_stages': 4, 'num_spks': 2, 'module_audio_enc': {'in_channels': 1, 'out_channels': 256, 'kernel_size': 16, 'stride': 4, 'groups': 1, 'bias': False}, 'module_feature_projector': {'num_channels': 256, 'in_channels': 256, 'out_channels': 128, 'kernel_size': 1, 'bias': False}, 'module_separator': {'num_stages': 4, 'relative_positional_encoding': {'in_channels': 128, 'num_heads': 8, 'maxlen': 2000, 'embed_v': False}, 'enc_stage': {'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'down_conv_layer': {'in_channels': 128, 'samp_kernel_size': 5}}, 'spk_split_stage': {'in_channels': 128, 'num_spks': 2}, 'simple_fusion': {'out_channels': 128}, 'dec_stage': {'num_spks': 2, 'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'spk_attention': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}}}, 'module_output_layer': {'in_channels': 256, 'out_channels': 128, 'num_spks': 2}, 'module_audio_dec': {'in_channels': 256, 'out_channels': 1, 'kernel_size': 16, 'stride': 4, 'bias': False}}, 'criterion': {'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, 'optimizer': {'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, 'scheduler': {'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, 'check_computations': {'dummy_len': 16000}, 'engine': {'max_epoch': 90, 'gpuid': '0', 'mvn': False, 'clip_norm': 5, 'start_scheduling': 10, 'test_epochs': [50, 90]}}})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.467\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m26\u001b[0m - \u001b[34m\u001b[1mEntering 'get_dataloaders' (args=(Namespace(model='SepReformer_Base_WSJ0', engine_mode='infer_sample', sample_file='/content/drive/MyDrive/×ª×•××¨/speech/mix_clean/mix_00046.wav', out_wav_dir='results_debug'), {'max_len': 32000, 'sampling_rate': 8000, 'scp_dir': 'data/custom', 'train': {'mixture': 'train_mix_clean.scp', 'spk1': 'train_s1.scp', 'spk2': 'train_s2.scp', 'dynamic_mixing': True}, 'valid': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}, 'test': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}}, {'batch_size': 1, 'pin_memory': True, 'num_workers': 4, 'drop_last': True}), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.467\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mEntering 'MyDataset' (args=(), kwargs={'max_len': 32000, 'fs': 8000, 'partition': 'train', 'wave_scp_srcs': ['data/custom/train_s1.scp', 'data/custom/train_s2.scp'], 'wave_scp_mix': 'data/custom/train_mix_clean.scp', 'dynamic_mixing': True})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mCreate MyDataset for data/custom/train_mix_clean.scp with 160 utterances\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.468\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mExiting 'MyDataset' (result=<models.SepReformer_Base_WSJ0.dataset.MyDataset object at 0x78f8fa9f3bc0>)\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.468\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mEntering 'MyDataset' (args=(), kwargs={'max_len': 32000, 'fs': 8000, 'partition': 'valid', 'wave_scp_srcs': ['data/custom/val_s1.scp', 'data/custom/val_s2.scp'], 'wave_scp_mix': 'data/custom/val_mix_clean.scp', 'dynamic_mixing': False})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mCreate MyDataset for data/custom/val_mix_clean.scp with 40 utterances\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.468\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mExiting 'MyDataset' (result=<models.SepReformer_Base_WSJ0.dataset.MyDataset object at 0x78f8f955f1d0>)\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.468\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mEntering 'MyDataset' (args=(), kwargs={'max_len': 32000, 'fs': 8000, 'partition': 'test', 'wave_scp_srcs': ['data/custom/val_s1.scp', 'data/custom/val_s2.scp'], 'wave_scp_mix': 'data/custom/val_mix_clean.scp', 'dynamic_mixing': False})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mCreate MyDataset for data/custom/val_mix_clean.scp with 40 utterances\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.469\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.dataset\u001b[0m:\u001b[36mget_dataloaders\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mExiting 'MyDataset' (result=<models.SepReformer_Base_WSJ0.dataset.MyDataset object at 0x78f8f95a00e0>)\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.469\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m26\u001b[0m - \u001b[34m\u001b[1mExiting 'get_dataloaders' (result={'train': <torch.utils.data.dataloader.DataLoader object at 0x78f9fbc186e0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x78fb3b714290>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x78f8f95a00b0>})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.469\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mEntering 'Model' (args=(), kwargs={'num_stages': 4, 'num_spks': 2, 'module_audio_enc': {'in_channels': 1, 'out_channels': 256, 'kernel_size': 16, 'stride': 4, 'groups': 1, 'bias': False}, 'module_feature_projector': {'num_channels': 256, 'in_channels': 256, 'out_channels': 128, 'kernel_size': 1, 'bias': False}, 'module_separator': {'num_stages': 4, 'relative_positional_encoding': {'in_channels': 128, 'num_heads': 8, 'maxlen': 2000, 'embed_v': False}, 'enc_stage': {'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'down_conv_layer': {'in_channels': 128, 'samp_kernel_size': 5}}, 'spk_split_stage': {'in_channels': 128, 'num_spks': 2}, 'simple_fusion': {'out_channels': 128}, 'dec_stage': {'num_spks': 2, 'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'spk_attention': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}}}, 'module_output_layer': {'in_channels': 256, 'out_channels': 128, 'num_spks': 2}, 'module_audio_dec': {'in_channels': 256, 'out_channels': 1, 'kernel_size': 16, 'stride': 4, 'bias': False}})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.640\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mExiting 'Model' (result=Model(\n",
            "  (audio_encoder): AudioEncoder(\n",
            "    (conv1d): Conv1d(1, 256, kernel_size=(16,), stride=(4,), bias=False)\n",
            "    (gelu): GELU(approximate='none')\n",
            "  )\n",
            "  (feature_projector): FeatureProjector(\n",
            "    (norm): GroupNorm(1, 256, eps=1e-08, affine=True)\n",
            "    (conv1d): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "  )\n",
            "  (separator): Separator(\n",
            "    (pos_emb): RelativePositionalEncoding(\n",
            "      (pe_k): Embedding(4000, 16)\n",
            "    )\n",
            "    (enc_stages): ModuleList(\n",
            "      (0-3): 4 x SepEncStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downconv): DownConvLayer(\n",
            "          (down_conv): Conv1d(128, 128, kernel_size=(5,), stride=(2,), padding=(2,), groups=128)\n",
            "          (BN): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (gelu): GELU(approximate='none')\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (bottleneck_G): SepEncStage(\n",
            "      (g_block_1): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_1): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (g_block_2): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_2): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (spk_split_block): SpkSplitStage(\n",
            "      (linear): Sequential(\n",
            "        (0): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
            "        (1): GLU(dim=-2)\n",
            "        (2): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
            "      )\n",
            "      (norm): GroupNorm(1, 128, eps=1e-08, affine=True)\n",
            "    )\n",
            "    (simple_fusion): ModuleList(\n",
            "      (0-3): 4 x Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "    (dec_stages): ModuleList(\n",
            "      (0-3): 4 x SepDecStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_1): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_2): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_3): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_3): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_3): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (out_layer): OutputLayer(\n",
            "    (spe_block): Masking(\n",
            "      (gate_act): ReLU()\n",
            "    )\n",
            "    (end_conv1x1): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (1): GLU(dim=-1)\n",
            "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (audio_decoder): AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  (out_layer_bn): ModuleList(\n",
            "    (0-3): 4 x OutputLayer(\n",
            "      (spe_block): Masking(\n",
            "        (gate_act): ReLU()\n",
            "      )\n",
            "      (end_conv1x1): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): GLU(dim=-1)\n",
            "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder_bn): ModuleList(\n",
            "    (0-3): 4 x AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  )\n",
            "))\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.661\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mEntering 'CriterionFactory' (args=({'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, device(type='cuda', index=0)), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.662\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m38\u001b[0m - \u001b[34m\u001b[1mExiting 'CriterionFactory' (result=CriterionFactory(config={'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, device=device(type='cuda', index=0)))\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SISNR_mag instance with args: {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.737\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SISNR_mag' (args=(device(type='cuda', index=0),), kwargs={'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:21.737\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.155\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.282\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.284\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.287\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.288\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.291\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mEntering 'STFT' (args=(device(type='cuda', index=0), 512, 128, 'hann'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.293\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.implements.criterions\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m132\u001b[0m - \u001b[34m\u001b[1mExiting 'STFT' (result=STFT(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', K=tensor([[[ 0.0000e+00,  1.3582e-06,  5.4340e-06,  ...,  1.2226e-05,\n",
            "           5.4340e-06,  1.3582e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3581e-06,  5.4324e-06,  ...,  1.2218e-05,\n",
            "           5.4324e-06,  1.3581e-06]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.3578e-06,  5.4274e-06,  ...,  1.2193e-05,\n",
            "           5.4274e-06,  1.3578e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0000e+00, -3.3333e-08,  2.6663e-07,  ...,  8.9942e-07,\n",
            "          -2.6663e-07,  3.3333e-08]],\n",
            "\n",
            "        [[ 0.0000e+00, -1.6668e-08,  1.3336e-07,  ...,  4.5001e-07,\n",
            "          -1.3336e-07,  1.6667e-08]],\n",
            "\n",
            "        [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]], device='cuda:0'), num_bins=257))\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.295\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SISNR_mag' (result=<PIT_SISNR_mag(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', num_stages=4, num_spks=2, scale_inv=True, mel_opt=False, stft = [STFT instance for 4 layers], mel_fb=Identity)>)\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SISNR_time instance with args: {'num_spks': 2, 'scale_inv': True}\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.296\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SISNR_time' (args=(device(type='cuda', index=0),), kwargs={'num_spks': 2, 'scale_inv': True})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.296\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SISNR_time' (result=<PIT_SISNR_time(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>)\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SISNRi instance with args: {'num_spks': 2, 'scale_inv': True}\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.296\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SISNRi' (args=(device(type='cuda', index=0),), kwargs={'num_spks': 2, 'scale_inv': True})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.296\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SISNRi' (result=<PIT_SISNRi(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>)\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating PIT_SDRi instance with args: {'dump': 0}\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.296\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'PIT_SDRi' (args=(device(type='cuda', index=0),), kwargs={'dump': 0})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.296\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'PIT_SDRi' (result=<PIT_SDRi(device=device(type='cuda', index=0), dump=0)>)\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.296\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mEntering 'OptimizerFactory' (args=({'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, <generator object Module.parameters at 0x78f8f91303c0>), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.296\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mExiting 'OptimizerFactory' (result=OptimizerFactory(config={'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, parameters_policy=<generator object Module.parameters at 0x78f8f91303c0>))\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating AdamW instance with args: {'lr': 0.001, 'weight_decay': 0.01}\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.301\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m40\u001b[0m - \u001b[34m\u001b[1mEntering 'SchedulerFactory' (args=({'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, [AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")]), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.301\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m40\u001b[0m - \u001b[34m\u001b[1mExiting 'SchedulerFactory' (result=SchedulerFactory(config={'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, optimizers=[AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")]))\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating ReduceLROnPlateau instance with args: {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mCreating WarmupConstantSchedule instance with args: {'warmup_steps': 100}\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.302\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mEntering 'WarmupConstantSchedule' (args=(AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "),), kwargs={'warmup_steps': 100})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.302\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mutils.util_implement\u001b[0m:\u001b[36mcreate_instance\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mExiting 'WarmupConstantSchedule' (result=<WarmupConstantSchedule(optimizer=AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.001\n",
            "    lr: 0.0\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "), warmup_steps=100, last_epoch=0, lr_lambda = <function WarmupConstantSchedule.__post_init__.<locals>.lr_lambda at 0x78f8f93d62a0>)>)\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.302\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mEntering 'Engine' (args=(Namespace(model='SepReformer_Base_WSJ0', engine_mode='infer_sample', sample_file='/content/drive/MyDrive/×ª×•××¨/speech/mix_clean/mix_00046.wav', out_wav_dir='results_debug'), {'dataset': {'max_len': 32000, 'sampling_rate': 8000, 'scp_dir': 'data/custom', 'train': {'mixture': 'train_mix_clean.scp', 'spk1': 'train_s1.scp', 'spk2': 'train_s2.scp', 'dynamic_mixing': True}, 'valid': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}, 'test': {'mixture': 'val_mix_clean.scp', 'spk1': 'val_s1.scp', 'spk2': 'val_s2.scp'}}, 'dataloader': {'batch_size': 1, 'pin_memory': True, 'num_workers': 4, 'drop_last': True}, 'model': {'num_stages': 4, 'num_spks': 2, 'module_audio_enc': {'in_channels': 1, 'out_channels': 256, 'kernel_size': 16, 'stride': 4, 'groups': 1, 'bias': False}, 'module_feature_projector': {'num_channels': 256, 'in_channels': 256, 'out_channels': 128, 'kernel_size': 1, 'bias': False}, 'module_separator': {'num_stages': 4, 'relative_positional_encoding': {'in_channels': 128, 'num_heads': 8, 'maxlen': 2000, 'embed_v': False}, 'enc_stage': {'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'down_conv_layer': {'in_channels': 128, 'samp_kernel_size': 5}}, 'spk_split_stage': {'in_channels': 128, 'num_spks': 2}, 'simple_fusion': {'out_channels': 128}, 'dec_stage': {'num_spks': 2, 'global_blocks': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}, 'local_blocks': {'in_channels': 128, 'kernel_size': 65, 'dropout_rate': 0.05}, 'spk_attention': {'in_channels': 128, 'num_mha_heads': 8, 'dropout_rate': 0.05}}}, 'module_output_layer': {'in_channels': 256, 'out_channels': 128, 'num_spks': 2}, 'module_audio_dec': {'in_channels': 256, 'out_channels': 1, 'kernel_size': 16, 'stride': 4, 'bias': False}}, 'criterion': {'name': ['PIT_SISNR_mag', 'PIT_SISNR_time', 'PIT_SISNRi', 'PIT_SDRi'], 'PIT_SISNR_mag': {'frame_length': 512, 'frame_shift': 128, 'window': 'hann', 'num_stages': 4, 'num_spks': 2, 'scale_inv': True, 'mel_opt': False}, 'PIT_SISNR_time': {'num_spks': 2, 'scale_inv': True}, 'PIT_SISNRi': {'num_spks': 2, 'scale_inv': True}, 'PIT_SDRi': {'dump': 0}}, 'optimizer': {'name': ['AdamW'], 'AdamW': {'lr': 0.001, 'weight_decay': 0.01}}, 'scheduler': {'name': ['ReduceLROnPlateau', 'WarmupConstantSchedule'], 'ReduceLROnPlateau': {'mode': 'min', 'min_lr': 1e-10, 'factor': 0.8, 'patience': 2}, 'WarmupConstantSchedule': {'warmup_steps': 100}}, 'check_computations': {'dummy_len': 16000}, 'engine': {'max_epoch': 90, 'gpuid': '0', 'mvn': False, 'clip_norm': 5, 'start_scheduling': 10, 'test_epochs': [50, 90]}}, Model(\n",
            "  (audio_encoder): AudioEncoder(\n",
            "    (conv1d): Conv1d(1, 256, kernel_size=(16,), stride=(4,), bias=False)\n",
            "    (gelu): GELU(approximate='none')\n",
            "  )\n",
            "  (feature_projector): FeatureProjector(\n",
            "    (norm): GroupNorm(1, 256, eps=1e-08, affine=True)\n",
            "    (conv1d): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "  )\n",
            "  (separator): Separator(\n",
            "    (pos_emb): RelativePositionalEncoding(\n",
            "      (pe_k): Embedding(4000, 16)\n",
            "    )\n",
            "    (enc_stages): ModuleList(\n",
            "      (0-3): 4 x SepEncStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downconv): DownConvLayer(\n",
            "          (down_conv): Conv1d(128, 128, kernel_size=(5,), stride=(2,), padding=(2,), groups=128)\n",
            "          (BN): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (gelu): GELU(approximate='none')\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (bottleneck_G): SepEncStage(\n",
            "      (g_block_1): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_1): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (g_block_2): GlobalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (ega): EGA(\n",
            "            (block): ModuleDict(\n",
            "              (self_attn): MultiHeadAttention(\n",
            "                (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (dropout): Dropout(p=0.05, inplace=False)\n",
            "                (Layer_scale): LayerScale()\n",
            "              )\n",
            "              (linear): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                (2): Sigmoid()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (l_block_2): LocalBlock(\n",
            "        (block): ModuleDict(\n",
            "          (cla): CLA(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (GLU): GLU(dim=-1)\n",
            "            (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "            (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "            (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (linear3): Sequential(\n",
            "              (0): GELU(approximate='none')\n",
            "              (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (2): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (gcfn): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (spk_split_block): SpkSplitStage(\n",
            "      (linear): Sequential(\n",
            "        (0): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
            "        (1): GLU(dim=-2)\n",
            "        (2): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
            "      )\n",
            "      (norm): GroupNorm(1, 128, eps=1e-08, affine=True)\n",
            "    )\n",
            "    (simple_fusion): ModuleList(\n",
            "      (0-3): 4 x Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
            "    )\n",
            "    (dec_stages): ModuleList(\n",
            "      (0-3): 4 x SepDecStage(\n",
            "        (g_block_1): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_1): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_1): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_2): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_2): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_2): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "        (g_block_3): GlobalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (ega): EGA(\n",
            "              (block): ModuleDict(\n",
            "                (self_attn): MultiHeadAttention(\n",
            "                  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (dropout): Dropout(p=0.05, inplace=False)\n",
            "                  (Layer_scale): LayerScale()\n",
            "                )\n",
            "                (linear): Sequential(\n",
            "                  (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "                  (2): Sigmoid()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (l_block_3): LocalBlock(\n",
            "          (block): ModuleDict(\n",
            "            (cla): CLA(\n",
            "              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (GLU): GLU(dim=-1)\n",
            "              (dw_conv_1d): Conv1d(128, 128, kernel_size=(65,), stride=(1,), padding=same, groups=128)\n",
            "              (linear2): Linear(in_features=128, out_features=256, bias=True)\n",
            "              (BN): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (linear3): Sequential(\n",
            "                (0): GELU(approximate='none')\n",
            "                (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (2): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "            (gcfn): GCFN(\n",
            "              (net1): Sequential(\n",
            "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "              )\n",
            "              (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "              (net2): Sequential(\n",
            "                (0): GLU(dim=-1)\n",
            "                (1): Dropout(p=0.05, inplace=False)\n",
            "                (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "                (3): Dropout(p=0.05, inplace=False)\n",
            "              )\n",
            "              (Layer_scale): LayerScale()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (spk_attn_3): SpkAttention(\n",
            "          (self_attn): MultiHeadAttention(\n",
            "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "            (linear_q): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_k): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_v): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.05, inplace=False)\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "          (feed_forward): GCFN(\n",
            "            (net1): Sequential(\n",
            "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "              (1): Linear(in_features=128, out_features=768, bias=True)\n",
            "            )\n",
            "            (depthwise): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
            "            (net2): Sequential(\n",
            "              (0): GLU(dim=-1)\n",
            "              (1): Dropout(p=0.05, inplace=False)\n",
            "              (2): Linear(in_features=384, out_features=128, bias=True)\n",
            "              (3): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (Layer_scale): LayerScale()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (out_layer): OutputLayer(\n",
            "    (spe_block): Masking(\n",
            "      (gate_act): ReLU()\n",
            "    )\n",
            "    (end_conv1x1): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "      (1): GLU(dim=-1)\n",
            "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (audio_decoder): AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  (out_layer_bn): ModuleList(\n",
            "    (0-3): 4 x OutputLayer(\n",
            "      (spe_block): Masking(\n",
            "        (gate_act): ReLU()\n",
            "      )\n",
            "      (end_conv1x1): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): GLU(dim=-1)\n",
            "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder_bn): ModuleList(\n",
            "    (0-3): 4 x AudioDecoder(256, 1, kernel_size=(16,), stride=(4,), bias=False)\n",
            "  )\n",
            "), {'train': <torch.utils.data.dataloader.DataLoader object at 0x78f9fbc186e0>, 'valid': <torch.utils.data.dataloader.DataLoader object at 0x78fb3b714290>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x78f8f95a00b0>}, [<PIT_SISNR_mag(device=device(type='cuda', index=0), frame_length=512, frame_shift=128, window='hann', num_stages=4, num_spks=2, scale_inv=True, mel_opt=False, stft = [STFT instance for 4 layers], mel_fb=Identity)>, <PIT_SISNR_time(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>, <PIT_SISNRi(device=device(type='cuda', index=0), num_spks=2, scale_inv=True)>, <PIT_SDRi(device=device(type='cuda', index=0), dump=0)>], [AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.001\n",
            "    lr: 0.0\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            ")], [<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x78f8f9135160>, <WarmupConstantSchedule(optimizer=AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.001\n",
            "    lr: 0.0\n",
            "    maximize: False\n",
            "    weight_decay: 0.01\n",
            "), warmup_steps=100, last_epoch=0, lr_lambda = <function WarmupConstantSchedule.__post_init__.<locals>.lr_lambda at 0x78f8f93d62a0>)>], (0,), device(type='cuda', index=0)), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:22.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_engine\u001b[0m:\u001b[36mload_last_checkpoint_n_get_epoch\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mLoaded Pretrained model from /content/SepReformer/models/SepReformer_Base_WSJ0/log/scratch_weights/epoch.0089.pth .....\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:24.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_engine\u001b[0m:\u001b[36mmodel_params_mac_summary\u001b[0m:\u001b[36m138\u001b[0m - \u001b[1mptflops: MACs: 45.13 GMac, Params: 14.69\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:24.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_engine\u001b[0m:\u001b[36mmodel_params_mac_summary\u001b[0m:\u001b[36m144\u001b[0m - \u001b[1mthop: MACs: 43.958357248 GMac, Params: 14.592\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:24.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.util_engine\u001b[0m:\u001b[36mmodel_params_mac_summary\u001b[0m:\u001b[36m150\u001b[0m - \u001b[1mtorchinfo: MACs: 5235.347456 GMac, Params: 14.691584\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:24.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.engine\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mClip gradient by 2-norm 5\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:24.939\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mExiting 'Engine' (result=<models.SepReformer_Base_WSJ0.engine.Engine object at 0x78f8f8adce60>)\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:24.939\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m45\u001b[0m - \u001b[34m\u001b[1mEntering '_inference_sample' (args=(<models.SepReformer_Base_WSJ0.engine.Engine object at 0x78f8f8adce60>, '/content/drive/MyDrive/×ª×•××¨/speech/mix_clean/mix_00046.wav'), kwargs={})\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:26.703\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mmodels.SepReformer_Base_WSJ0.main\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m45\u001b[0m - \u001b[34m\u001b[1mExiting '_inference_sample' (result=None)\u001b[0m\n",
            "\u001b[32m2026-02-15 17:07:26.703\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m32\u001b[0m - \u001b[34m\u001b[1mExiting 'main' (result=None)\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define Paths\n",
        "checkpoint_path = \"/content/SepReformer/models/SepReformer_Base_WSJ0/log/scratch_weights/\"\n",
        "# Find the latest checkpoint automatically\n",
        "checkpoints = sorted(glob.glob(os.path.join(checkpoint_path, \"epoch.*.pth\")))\n",
        "latest_ckpt = checkpoints[-1]\n",
        "print(f\"â­ Using Checkpoint: {os.path.basename(latest_ckpt)}\")\n",
        "\n",
        "# 2. Pick a random test file\n",
        "mix_dir = '/content/drive/MyDrive/×ª×•××¨/speech/mix_clean'\n",
        "files = [f for f in os.listdir(mix_dir) if f.endswith('.wav') and '_in' not in f and '_out' not in f]\n",
        "test_file = random.choice(files)\n",
        "test_file_path = os.path.join(mix_dir, test_file)\n",
        "\n",
        "print(f\"ğŸµ Separating: {test_file}\")\n",
        "\n",
        "# 3. Run Inference\n",
        "!python run.py \\\n",
        "  --model SepReformer_Base_WSJ0 \\\n",
        "  --engine-mode infer_sample \\\n",
        "  --sample-file \"{test_file_path}\" \\\n",
        "  --out-wav-dir \"final_results_90ep\" > /dev/null 2>&1\n",
        "\n",
        "# 4. Locate and Move Files (Handling the path bug we saw earlier)\n",
        "# The model likely saved them in the source folder again due to the bug\n",
        "base_name = os.path.splitext(test_file)[0]\n",
        "path_mix = os.path.join(mix_dir, f\"{base_name}_in.wav\")\n",
        "path_s1 = os.path.join(mix_dir, f\"{base_name}_out_0.wav\")\n",
        "path_s2 = os.path.join(mix_dir, f\"{base_name}_out_1.wav\")\n",
        "\n",
        "if os.path.exists(path_s1):\n",
        "    print(\"âœ… Separation successful!\\n\")\n",
        "\n",
        "    # Audio Player\n",
        "    print(\"ğŸ§ Original Mixture:\")\n",
        "    ipd.display(ipd.Audio(path_mix))\n",
        "\n",
        "    print(\"ğŸ—£ï¸ Speaker 1 (Est):\")\n",
        "    ipd.display(ipd.Audio(path_s1))\n",
        "\n",
        "    print(\"ğŸ—£ï¸ Speaker 2 (Est):\")\n",
        "    ipd.display(ipd.Audio(path_s2))\n",
        "\n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    def plot_spec(ax, p, title):\n",
        "        y, sr = librosa.load(p, sr=8000)\n",
        "        D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
        "        img = librosa.display.specshow(D, y_axis='linear', x_axis='time', sr=sr, ax=ax)\n",
        "        ax.set_title(title)\n",
        "        return img\n",
        "\n",
        "    img = plot_spec(ax[0], path_mix, \"Mixture\")\n",
        "    plot_spec(ax[1], path_s1, \"Speaker 1\")\n",
        "    plot_spec(ax[2], path_s2, \"Speaker 2\")\n",
        "    fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Output files not found. Check if the model saved to a different location.\")"
      ],
      "metadata": {
        "id": "XQs_szti7zZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **save model offline**"
      ],
      "metadata": {
        "id": "r5zbBMC6tOSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "# 1. Define Paths\n",
        "# Source directories\n",
        "model_root = \"/content/SepReformer/models/SepReformer_Base_WSJ0\"\n",
        "weights_dir = os.path.join(model_root, \"log\", \"scratch_weights\")\n",
        "\n",
        "# Destination directory in Drive (Persistent storage)\n",
        "dest_dir = \"/content/drive/MyDrive/×ª×•××¨/speech/SepReformer_Saved_Model\"\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "# 2. Identify Files to Save\n",
        "# A. Latest Checkpoint\n",
        "checkpoints = sorted(glob.glob(os.path.join(weights_dir, \"epoch.*.pth\")))\n",
        "if not checkpoints:\n",
        "    print(\"âŒ No checkpoints found!\")\n",
        "else:\n",
        "    latest_ckpt = checkpoints[-1]\n",
        "    ckpt_name = os.path.basename(latest_ckpt)\n",
        "\n",
        "    # B. Config and Patched Code\n",
        "    files_to_save = {\n",
        "        latest_ckpt: os.path.join(dest_dir, ckpt_name),\n",
        "        os.path.join(model_root, \"configs.yaml\"): os.path.join(dest_dir, \"configs.yaml\"),\n",
        "        os.path.join(model_root, \"dataset.py\"): os.path.join(dest_dir, \"dataset.py\"), # Save patched file\n",
        "        os.path.join(model_root, \"engine.py\"): os.path.join(dest_dir, \"engine.py\")   # Save patched file\n",
        "    }\n",
        "\n",
        "    print(f\"ğŸ’¾ Saving files to: {dest_dir} ...\")\n",
        "\n",
        "    # 3. Copy Files\n",
        "    for src, dst in files_to_save.items():\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy2(src, dst)\n",
        "            print(f\"âœ… Saved: {os.path.basename(src)}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ Warning: Source file not found: {src}\")\n"
      ],
      "metadata": {
        "id": "0icg8CPS8lfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **load model**"
      ],
      "metadata": {
        "id": "QXR9f0m6tUbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Mount Drive ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 2. Setup Repository (Clone & Install) ---\n",
        "if not os.path.exists('/content/SepReformer'):\n",
        "    print(\"ğŸš€ Cloning SepReformer...\")\n",
        "    !git clone https://github.com/dmlguq456/SepReformer.git\n",
        "    %cd SepReformer\n",
        "    print(\"ğŸ“¦ Installing dependencies...\")\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install loguru torchinfo ptflops thop mir_eval\n",
        "else:\n",
        "    print(\"âœ… Repository already exists.\")\n",
        "    %cd /content/SepReformer\n",
        "\n",
        "# --- 3. Restore Saved Model ---\n",
        "# Paths\n",
        "saved_models_dir = '/content/drive/MyDrive/×ª×•××¨/speech/SepReformer_Saved_Model'\n",
        "model_root = 'models/SepReformer_Base_WSJ0' # Relative to /content/SepReformer\n",
        "\n",
        "print(f\"ğŸ”„ Restoring model files from: {saved_models_dir}\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(os.path.join(model_root, 'log', 'scratch_weights'), exist_ok=True)\n",
        "\n",
        "# Files to restore\n",
        "files_to_copy = ['configs.yaml', 'dataset.py', 'engine.py']\n",
        "\n",
        "for fname in files_to_copy:\n",
        "    src = os.path.join(saved_models_dir, fname)\n",
        "    dst = os.path.join(model_root, fname)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy2(src, dst)\n",
        "        print(f\"  - Restored {fname}\")\n",
        "    else:\n",
        "        print(f\"  âš ï¸ Missing {fname} in saved dir\")\n",
        "\n",
        "# Restore Checkpoint\n",
        "saved_ckpts = sorted(glob.glob(os.path.join(saved_models_dir, \"epoch.*.pth\")))\n",
        "if saved_ckpts:\n",
        "    latest_ckpt = saved_ckpts[-1]\n",
        "    ckpt_name = os.path.basename(latest_ckpt)\n",
        "    dst_ckpt = os.path.join(model_root, 'log', 'scratch_weights', ckpt_name)\n",
        "    shutil.copy2(latest_ckpt, dst_ckpt)\n",
        "    print(f\"  - Restored Checkpoint: {ckpt_name}\")\n",
        "else:\n",
        "    print(\"  âŒ No checkpoint found to restore!\")\n",
        "\n",
        "print(\"\\nğŸ‰ Ready! The environment is set up and the model is loaded.\")"
      ],
      "metadata": {
        "id": "LkGZAzLzrSiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Attention model**"
      ],
      "metadata": {
        "id": "M4Y5K8e3tacE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ks4np44ktw0D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}